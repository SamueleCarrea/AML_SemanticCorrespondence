{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNR4LpdBLrhl"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELLA 1: Setup Progetto (riutilizza da eval.ipynb)\n",
        "# ============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\" AML Semantic Correspondence - Fine-tuning Stage\\n\")\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "if not Path('/content/drive').exists():\n",
        "    drive.mount('/content/drive')\n",
        "    print(\" Google Drive mounted\\n\")\n",
        "else:\n",
        "    print(\" Google Drive already mounted\\n\")\n",
        "\n",
        "# 2. Setup directories\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/AML'\n",
        "DATA_DIR = f'{PROJECT_ROOT}/dataset'\n",
        "CHECKPOINT_DIR = f'{PROJECT_ROOT}/checkpoints'\n",
        "RESULTS_DIR = f'{PROJECT_ROOT}/results'\n",
        "FINETUNED_DIR = f'{PROJECT_ROOT}/finetuned_models'\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(FINETUNED_DIR, exist_ok=True)\n",
        "\n",
        "# 3. Clone repository\n",
        "GITHUB_REPO_URL = 'https://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@github.com/SamueleCarrea/AML_SemanticCorrespondence'\n",
        "LOCAL_REPO_NAME = 'AML_SemanticCorrespondence'\n",
        "\n",
        "if not Path(LOCAL_REPO_NAME).exists():\n",
        "    print(f\"\\n Cloning repository...\")\n",
        "    !git clone {GITHUB_REPO_URL} {LOCAL_REPO_NAME}\n",
        "    print(\" Repository cloned\")\n",
        "else:\n",
        "    print(f\"\\n Repository {LOCAL_REPO_NAME} already exists.\")\n",
        "    if Path(LOCAL_REPO_NAME, '.git').exists():\n",
        "        print(\" Pulling latest changes...\")\n",
        "        %cd {LOCAL_REPO_NAME}\n",
        "        !git pull\n",
        "        %cd ..\n",
        "        print(\" Repository updated\")\n",
        "\n",
        "sys.path.insert(0, LOCAL_REPO_NAME)\n",
        "\n",
        "# 4. GPU info\n",
        "import torch\n",
        "print(f\"\\n  GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\" VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"\\n Setup complete!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 2: Install Dependencies\n",
        "# ============================================================================\n",
        "\n",
        "print(\" Installing dependencies...\\n\")\n",
        "\n",
        "!pip install -q -r {LOCAL_REPO_NAME}/requirements.txt\n",
        "!pip install -q tensorboard\n",
        "\n",
        "import torch\n",
        "print(f\"\\n PyTorch {torch.__version__}\")\n",
        "print(f\" CUDA available: {torch.cuda.is_available()}\")\n",
        "print(\"\\n Dependencies installed!\\n\")"
      ],
      "metadata": {
        "id": "Cv5vl0bTNhv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 3: Load Datasets (Train, Val, Test)\n",
        "# ============================================================================\n",
        "\n",
        "from dataset.spair import SPairDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "SPAIR_ROOT = f'{DATA_DIR}/Spair-71k'\n",
        "\n",
        "# Load all splits\n",
        "train_dataset = SPairDataset(\n",
        "    root=SPAIR_ROOT,\n",
        "    split='train',\n",
        "    size='large',\n",
        "    long_side=518,\n",
        "    normalize=True,\n",
        "    load_segmentation=False\n",
        ")\n",
        "\n",
        "val_dataset = SPairDataset(\n",
        "    root=SPAIR_ROOT,\n",
        "    split='val',\n",
        "    size='large',\n",
        "    long_side=518,\n",
        "    normalize=True,\n",
        "    load_segmentation=False\n",
        ")\n",
        "\n",
        "test_dataset = SPairDataset(\n",
        "    root=SPAIR_ROOT,\n",
        "    split='test',\n",
        "    size='large',\n",
        "    long_side=518,\n",
        "    normalize=True,\n",
        "    load_segmentation=False\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\" Dataset Statistics:\")\n",
        "print(f\"   Train: {len(train_dataset)} pairs\")\n",
        "print(f\"   Val:   {len(val_dataset)} pairs\")\n",
        "print(f\"   Test:  {len(test_dataset)} pairs\")\n",
        "print(f\"\\n Datasets loaded!\")"
      ],
      "metadata": {
        "id": "g_o7DroGNil-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 4: Fine-tuning Configuration\n",
        "# ============================================================================\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class FinetuneConfig:\n",
        "    \"\"\"Fine-tuning configuration.\"\"\"\n",
        "\n",
        "    # Model\n",
        "    backbone_name: str = 'dinov2_vitb14'\n",
        "    num_layers_to_finetune: int = 2  # Number of last transformer blocks\n",
        "\n",
        "    # Training\n",
        "    num_epochs: int = 10\n",
        "    learning_rate: float = 1e-5\n",
        "    weight_decay: float = 0.01\n",
        "    warmup_epochs: int = 1\n",
        "\n",
        "    # Loss\n",
        "    loss_type: str = 'cosine'  # 'cosine', 'l2', or 'combined'\n",
        "    negative_margin: float = 0.2\n",
        "\n",
        "    # Optimization\n",
        "    batch_size: int = 8\n",
        "    gradient_accumulation_steps: int = 1\n",
        "    max_grad_norm: float = 1.0\n",
        "\n",
        "    # Logging\n",
        "    log_interval: int = 50\n",
        "    val_interval: int = 500\n",
        "    save_interval: int = 1000\n",
        "\n",
        "    # Paths\n",
        "    checkpoint_dir: str = FINETUNED_DIR\n",
        "    experiment_name: str = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.experiment_name is None:\n",
        "            self.experiment_name = f\"{self.backbone_name}_ft{self.num_layers_to_finetune}\"\n",
        "\n",
        "\n",
        "# Create configs for different experiments\n",
        "configs = {\n",
        "    'dinov2_2layers': FinetuneConfig(\n",
        "        backbone_name='dinov2_vitb14',\n",
        "        num_layers_to_finetune=2,\n",
        "        num_epochs=10,\n",
        "        learning_rate=1e-5\n",
        "    ),\n",
        "    'dinov2_4layers': FinetuneConfig(\n",
        "        backbone_name='dinov2_vitb14',\n",
        "        num_layers_to_finetune=4,\n",
        "        num_epochs=10,\n",
        "        learning_rate=5e-6\n",
        "    ),\n",
        "    'dinov3_2layers': FinetuneConfig(\n",
        "        backbone_name='dinov3_vitb16',\n",
        "        num_layers_to_finetune=2,\n",
        "        num_epochs=10,\n",
        "        learning_rate=1e-5\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Select config for this run\n",
        "config = configs['dinov2_2layers']\n",
        "\n",
        "print(f\"   Fine-tuning Configuration:\")\n",
        "print(f\"   Backbone: {config.backbone_name}\")\n",
        "print(f\"   Layers to finetune: {config.num_layers_to_finetune}\")\n",
        "print(f\"   Epochs: {config.num_epochs}\")\n",
        "print(f\"   Learning rate: {config.learning_rate}\")\n",
        "print(f\"   Batch size: {config.batch_size}\")\n",
        "print(f\"\\n Config ready!\")"
      ],
      "metadata": {
        "id": "83Y2Za4pNnIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 5: Loss Functions for Correspondence\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CorrespondenceLoss(nn.Module):\n",
        "    \"\"\"Loss function for semantic correspondence fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        loss_type: str = 'cosine',\n",
        "        negative_margin: float = 0.2,\n",
        "        temperature: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_type = loss_type\n",
        "        self.negative_margin = negative_margin\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src_features: torch.Tensor,\n",
        "        tgt_features: torch.Tensor,\n",
        "        src_kps: torch.Tensor,\n",
        "        tgt_kps: torch.Tensor,\n",
        "        valid_mask: torch.Tensor,\n",
        "        patch_size: int\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute correspondence loss.\n",
        "\n",
        "        Args:\n",
        "            src_features: (B, H_s, W_s, D)\n",
        "            tgt_features: (B, H_t, W_t, D)\n",
        "            src_kps: (B, N, 2) source keypoints\n",
        "            tgt_kps: (B, N, 2) target keypoints\n",
        "            valid_mask: (B, N) valid keypoint mask\n",
        "            patch_size: int\n",
        "\n",
        "        Returns:\n",
        "            loss: scalar tensor\n",
        "        \"\"\"\n",
        "        B = src_features.shape[0]\n",
        "        total_loss = 0.0\n",
        "        n_valid = 0\n",
        "\n",
        "        for b in range(B):\n",
        "            src_feat = src_features[b]  # (H_s, W_s, D)\n",
        "            tgt_feat = tgt_features[b]  # (H_t, W_t, D)\n",
        "            src_kp = src_kps[b]  # (N, 2)\n",
        "            tgt_kp = tgt_kps[b]  # (N, 2)\n",
        "            valid = valid_mask[b]  # (N,)\n",
        "\n",
        "            if valid.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            # Filter valid keypoints\n",
        "            src_kp_valid = src_kp[valid]\n",
        "            tgt_kp_valid = tgt_kp[valid]\n",
        "\n",
        "            # Convert to patch coordinates\n",
        "            src_kp_patch = (src_kp_valid / patch_size).long()\n",
        "            tgt_kp_patch = (tgt_kp_valid / patch_size).long()\n",
        "\n",
        "            H_s, W_s, D = src_feat.shape\n",
        "            H_t, W_t, _ = tgt_feat.shape\n",
        "\n",
        "            # Clamp coordinates\n",
        "            src_kp_patch[:, 0] = src_kp_patch[:, 0].clamp(0, W_s - 1)\n",
        "            src_kp_patch[:, 1] = src_kp_patch[:, 1].clamp(0, H_s - 1)\n",
        "            tgt_kp_patch[:, 0] = tgt_kp_patch[:, 0].clamp(0, W_t - 1)\n",
        "            tgt_kp_patch[:, 1] = tgt_kp_patch[:, 1].clamp(0, H_t - 1)\n",
        "\n",
        "            # Extract features at keypoint locations\n",
        "            N = src_kp_valid.shape[0]\n",
        "            for i in range(N):\n",
        "                src_x, src_y = src_kp_patch[i]\n",
        "                tgt_x, tgt_y = tgt_kp_patch[i]\n",
        "\n",
        "                src_vec = src_feat[src_y, src_x]  # (D,)\n",
        "                tgt_vec = tgt_feat[tgt_y, tgt_x]  # (D,)\n",
        "\n",
        "                if self.loss_type == 'cosine':\n",
        "                    # Maximize cosine similarity between corresponding points\n",
        "                    similarity = F.cosine_similarity(\n",
        "                        src_vec.unsqueeze(0), tgt_vec.unsqueeze(0), dim=1\n",
        "                    )\n",
        "                    loss = 1.0 - similarity\n",
        "\n",
        "                elif self.loss_type == 'l2':\n",
        "                    # Minimize L2 distance\n",
        "                    loss = F.mse_loss(src_vec, tgt_vec)\n",
        "\n",
        "                elif self.loss_type == 'contrastive':\n",
        "                    # Contrastive loss: pull positives, push negatives\n",
        "                    # Positive: corresponding point\n",
        "                    pos_sim = F.cosine_similarity(\n",
        "                        src_vec.unsqueeze(0), tgt_vec.unsqueeze(0), dim=1\n",
        "                    )\n",
        "\n",
        "                    # Negatives: sample random points from target\n",
        "                    neg_indices = torch.randint(0, H_t * W_t, (8,), device=src_vec.device)\n",
        "                    tgt_flat = tgt_feat.reshape(-1, D)\n",
        "                    neg_vecs = tgt_flat[neg_indices]  # (8, D)\n",
        "\n",
        "                    neg_sim = F.cosine_similarity(\n",
        "                        src_vec.unsqueeze(0).expand(8, -1),\n",
        "                        neg_vecs,\n",
        "                        dim=1\n",
        "                    )\n",
        "\n",
        "                    # InfoNCE-style loss\n",
        "                    pos_exp = torch.exp(pos_sim / self.temperature)\n",
        "                    neg_exp = torch.exp(neg_sim / self.temperature).sum()\n",
        "\n",
        "                    loss = -torch.log(pos_exp / (pos_exp + neg_exp))\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n",
        "\n",
        "                total_loss += loss\n",
        "                n_valid += 1\n",
        "\n",
        "        if n_valid == 0:\n",
        "            return torch.tensor(0.0, device=src_features.device, requires_grad=True)\n",
        "\n",
        "        return total_loss / n_valid\n",
        "\n",
        "\n",
        "# Test loss function\n",
        "print(\" Testing loss function...\")\n",
        "\n",
        "loss_fn = CorrespondenceLoss(loss_type='cosine')\n",
        "\n",
        "# Dummy data\n",
        "src_feat = torch.randn(2, 37, 37, 768)\n",
        "tgt_feat = torch.randn(2, 37, 37, 768)\n",
        "src_kps = torch.randint(0, 500, (2, 10, 2)).float()\n",
        "tgt_kps = torch.randint(0, 500, (2, 10, 2)).float()\n",
        "valid_mask = torch.ones(2, 10).bool()\n",
        "\n",
        "loss = loss_fn(src_feat, tgt_feat, src_kps, tgt_kps, valid_mask, patch_size=14)\n",
        "print(f\" Loss computed: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "GQXj_zxxNq4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 6: Trainable Backbone Wrapper\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from models.backbones import DINOv2Extractor, DINOv3Extractor\n",
        "\n",
        "class FinetunableBackbone(nn.Module):\n",
        "    \"\"\"Wrapper to make backbone partially trainable.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone_name: str,\n",
        "        num_layers_to_finetune: int = 2,\n",
        "        device: str = 'cuda'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.backbone_name = backbone_name\n",
        "        self.num_layers_to_finetune = num_layers_to_finetune\n",
        "        self.device = device\n",
        "\n",
        "        # Load frozen backbone\n",
        "        if 'dinov2' in backbone_name:\n",
        "            self.extractor = DINOv2Extractor(\n",
        "                variant=backbone_name,\n",
        "                device=device,\n",
        "                allow_hub_download=True\n",
        "            )\n",
        "        elif 'dinov3' in backbone_name:\n",
        "            self.extractor = DINOv3Extractor(\n",
        "                variant=backbone_name,\n",
        "                device=device\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
        "\n",
        "        self.stride = self.extractor.stride\n",
        "\n",
        "        # Freeze all parameters first\n",
        "        for param in self.extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze last N transformer blocks\n",
        "        self._unfreeze_last_layers(num_layers_to_finetune)\n",
        "\n",
        "        # Count trainable parameters\n",
        "        n_trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        n_total = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "        print(f\"\\n Trainable parameters:\")\n",
        "        print(f\"   Total: {n_total:,}\")\n",
        "        print(f\"   Trainable: {n_trainable:,} ({n_trainable/n_total*100:.2f}%)\")\n",
        "\n",
        "    def _unfreeze_last_layers(self, num_layers: int):\n",
        "        \"\"\"Unfreeze last N transformer blocks.\"\"\"\n",
        "        model = self.extractor.model\n",
        "\n",
        "        # Access transformer blocks\n",
        "        if hasattr(model, 'blocks'):\n",
        "            blocks = model.blocks\n",
        "        elif hasattr(model, 'encoder') and hasattr(model.encoder, 'layers'):\n",
        "            blocks = model.encoder.layers\n",
        "        else:\n",
        "            raise AttributeError(\"Cannot find transformer blocks in model\")\n",
        "\n",
        "        total_blocks = len(blocks)\n",
        "        start_idx = max(0, total_blocks - num_layers)\n",
        "\n",
        "        print(f\"\\n Unfreezing blocks {start_idx} to {total_blocks-1} (total: {total_blocks})\")\n",
        "\n",
        "        for i in range(start_idx, total_blocks):\n",
        "            for param in blocks[i].parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def extract_features(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Extract features (with gradient if training).\"\"\"\n",
        "        feat_map, stride = self.extractor.extract_feats(image)\n",
        "        features = feat_map.permute(0, 2, 3, 1)  # (B, H, W, D)\n",
        "        return features\n",
        "\n",
        "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        return self.extract_features(image)\n",
        "\n",
        "\n",
        "# Test trainable backbone\n",
        "print(\" Testing trainable backbone...\")\n",
        "\n",
        "backbone = FinetunableBackbone(\n",
        "    backbone_name='dinov2_vitb14',\n",
        "    num_layers_to_finetune=2,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "test_img = torch.randn(1, 3, 518, 518).to(backbone.device)\n",
        "with torch.set_grad_enabled(True):\n",
        "    features = backbone(test_img)\n",
        "    print(f\"\\n Features shape: {features.shape}\")\n",
        "    print(f\" Requires grad: {features.requires_grad}\")"
      ],
      "metadata": {
        "id": "_OtVU3wGNuqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 7: Training Loop\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Fine-tuning trainer.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: FinetunableBackbone,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        config: FinetuneConfig,\n",
        "        device: str = 'cuda'\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = CorrespondenceLoss(\n",
        "            loss_type=config.loss_type,\n",
        "            negative_margin=config.negative_margin\n",
        "        ).to(device)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.AdamW(\n",
        "            [p for p in model.parameters() if p.requires_grad],\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler with warmup\n",
        "        warmup_steps = config.warmup_epochs * len(train_loader)\n",
        "        total_steps = config.num_epochs * len(train_loader)\n",
        "\n",
        "        warmup_scheduler = LinearLR(\n",
        "            self.optimizer,\n",
        "            start_factor=0.1,\n",
        "            total_iters=warmup_steps\n",
        "        )\n",
        "\n",
        "        cosine_scheduler = CosineAnnealingLR(\n",
        "            self.optimizer,\n",
        "            T_max=total_steps - warmup_steps\n",
        "        )\n",
        "\n",
        "        self.scheduler = SequentialLR(\n",
        "            self.optimizer,\n",
        "            schedulers=[warmup_scheduler, cosine_scheduler],\n",
        "            milestones=[warmup_steps]\n",
        "        )\n",
        "\n",
        "        # Logging\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.global_step = 0\n",
        "\n",
        "        # Checkpoint directory\n",
        "        self.ckpt_dir = Path(config.checkpoint_dir) / config.experiment_name\n",
        "        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(f\"\\n  Checkpoints: {self.ckpt_dir}\")\n",
        "\n",
        "    def train_epoch(self, epoch: int):\n",
        "        \"\"\"Train for one epoch.\"\"\"\n",
        "        self.model.train()\n",
        "        epoch_loss = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            # Move to device\n",
        "            src_img = batch['src_img'].to(self.device)\n",
        "            tgt_img = batch['tgt_img'].to(self.device)\n",
        "            src_kps = batch['src_kps'].to(self.device)\n",
        "            tgt_kps = batch['tgt_kps'].to(self.device)\n",
        "            valid_mask = batch['valid_mask'].to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            src_features = self.model(src_img)\n",
        "            tgt_features = self.model(tgt_img)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.criterion(\n",
        "                src_features, tgt_features,\n",
        "                src_kps, tgt_kps, valid_mask,\n",
        "                patch_size=self.model.stride\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            loss = loss / self.config.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    self.model.parameters(),\n",
        "                    self.config.max_grad_norm\n",
        "                )\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "            # Logging\n",
        "            epoch_loss += loss.item() * self.config.gradient_accumulation_steps\n",
        "            n_batches += 1\n",
        "            self.global_step += 1\n",
        "\n",
        "            if self.global_step % self.config.log_interval == 0:\n",
        "                avg_loss = epoch_loss / n_batches\n",
        "                lr = self.optimizer.param_groups[0]['lr']\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{avg_loss:.4f}',\n",
        "                    'lr': f'{lr:.2e}'\n",
        "                })\n",
        "                self.train_losses.append({\n",
        "                    'step': self.global_step,\n",
        "                    'loss': avg_loss,\n",
        "                    'lr': lr\n",
        "                })\n",
        "\n",
        "            # Validation\n",
        "            if self.global_step % self.config.val_interval == 0:\n",
        "                val_loss = self.validate()\n",
        "                self.val_losses.append({\n",
        "                    'step': self.global_step,\n",
        "                    'loss': val_loss\n",
        "                })\n",
        "                self.model.train()\n",
        "\n",
        "            # Save checkpoint\n",
        "            if self.global_step % self.config.save_interval == 0:\n",
        "                self.save_checkpoint(epoch, batch_idx)\n",
        "\n",
        "        return epoch_loss / n_batches\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self):\n",
        "        \"\"\"Validation loop.\"\"\"\n",
        "        self.model.eval()\n",
        "        val_loss = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        for batch in tqdm(self.val_loader, desc=\"Validation\", leave=False):\n",
        "            src_img = batch['src_img'].to(self.device)\n",
        "            tgt_img = batch['tgt_img'].to(self.device)\n",
        "            src_kps = batch['src_kps'].to(self.device)\n",
        "            tgt_kps = batch['tgt_kps'].to(self.device)\n",
        "            valid_mask = batch['valid_mask'].to(self.device)\n",
        "\n",
        "            src_features = self.model(src_img)\n",
        "            tgt_features = self.model(tgt_img)\n",
        "\n",
        "            loss = self.criterion(\n",
        "                src_features, tgt_features,\n",
        "                src_kps, tgt_kps, valid_mask,\n",
        "                patch_size=self.model.stride\n",
        "            )\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            n_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss / n_batches\n",
        "        print(f\"\\n Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        return avg_val_loss\n",
        "\n",
        "    def save_checkpoint(self, epoch: int, batch_idx: int):\n",
        "        \"\"\"Save training checkpoint.\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'global_step': self.global_step,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'config': self.config,\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "        }\n",
        "\n",
        "        ckpt_path = self.ckpt_dir / f'checkpoint_step{self.global_step}.pt'\n",
        "        torch.save(checkpoint, ckpt_path)\n",
        "        print(f\"\\n Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "        # Save config\n",
        "        config_path = self.ckpt_dir / 'config.json'\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(vars(self.config), f, indent=2)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Full training loop.\"\"\"\n",
        "        print(f\"\\n  Starting training: {self.config.experiment_name}\")\n",
        "        print(f\"   Epochs: {self.config.num_epochs}\")\n",
        "        print(f\"   Batches per epoch: {len(self.train_loader)}\")\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            train_loss = self.train_epoch(epoch)\n",
        "            print(f\"\\n Epoch {epoch+1} - Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "            # End-of-epoch validation\n",
        "            val_loss = self.validate()\n",
        "\n",
        "            # Save epoch checkpoint\n",
        "            self.save_checkpoint(epoch, len(self.train_loader))\n",
        "\n",
        "        print(f\"\\n Training complete!\")\n",
        "\n",
        "\n",
        "print(\" Trainer ready!\")"
      ],
      "metadata": {
        "id": "fwDlfuheNxsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 8: Launch Fine-tuning\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = FinetunableBackbone(\n",
        "    backbone_name=config.backbone_name,\n",
        "    num_layers_to_finetune=config.num_layers_to_finetune,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    config=config,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Kwc7dQ3nN1m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 9: Evaluate Fine-tuned Model\n",
        "# ============================================================================\n",
        "\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "class FinetunedEvaluator:\n",
        "    \"\"\"Evaluator for fine-tuned models.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: FinetunableBackbone,\n",
        "        dataloader: DataLoader,\n",
        "        device: str = 'cuda',\n",
        "        thresholds: list = [0.05, 0.10, 0.15, 0.20]\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.dataloader = dataloader\n",
        "        self.device = device\n",
        "        self.thresholds = thresholds\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, num_samples: int = None):\n",
        "        \"\"\"Evaluate fine-tuned model.\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        all_pck = defaultdict(list)\n",
        "        per_category = defaultdict(lambda: defaultdict(list))\n",
        "        inference_times = []\n",
        "\n",
        "        n_processed = 0\n",
        "        pbar = tqdm(self.dataloader, desc=\"Evaluating\")\n",
        "\n",
        "        for batch in pbar:\n",
        "            if num_samples and n_processed >= num_samples:\n",
        "                break\n",
        "\n",
        "            src_img = batch['src_img'].to(self.device)\n",
        "            tgt_img = batch['tgt_img'].to(self.device)\n",
        "            src_kps = batch['src_kps'][0]\n",
        "            tgt_kps = batch['tgt_kps'][0]\n",
        "            valid_mask = batch['valid_mask'][0]\n",
        "            category = batch['category'][0]\n",
        "\n",
        "            src_kps_valid = src_kps[valid_mask]\n",
        "            tgt_kps_valid = tgt_kps[valid_mask]\n",
        "\n",
        "            if len(src_kps_valid) == 0:\n",
        "                continue\n",
        "\n",
        "            # Predict\n",
        "            start = time.time()\n",
        "            tgt_kps_pred = self._predict_keypoints(\n",
        "                src_img, tgt_img, src_kps_valid\n",
        "            )\n",
        "            inference_times.append(time.time() - start)\n",
        "\n",
        "            # Compute metrics\n",
        "            from dataset.spair import compute_pck\n",
        "            H, W = tgt_img.shape[2:]\n",
        "            pck_scores = compute_pck(\n",
        "                tgt_kps_pred, tgt_kps_valid, (H, W), thresholds=self.thresholds\n",
        "            )\n",
        "\n",
        "            for metric, value in pck_scores.items():\n",
        "                all_pck[metric].append(value)\n",
        "                per_category[category][metric].append(value)\n",
        "\n",
        "            n_processed += 1\n",
        "\n",
        "            if len(all_pck['PCK@0.10']) > 0:\n",
        "                avg_pck = np.mean(all_pck['PCK@0.10'])\n",
        "                pbar.set_postfix({'PCK@0.10': f'{avg_pck:.4f}'})\n",
        "\n",
        "        # Aggregate results\n",
        "        results = {\n",
        "            'overall': {},\n",
        "            'per_category': {},\n",
        "            'num_pairs': n_processed,\n",
        "            'inference_time_ms': np.mean(inference_times) * 1000\n",
        "        }\n",
        "\n",
        "        for metric in [f'PCK@{t:.2f}' for t in self.thresholds]:\n",
        "            values = all_pck[metric]\n",
        "            results['overall'][metric] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values)\n",
        "            }\n",
        "\n",
        "        for cat, metrics in per_category.items():\n",
        "            results['per_category'][cat] = {}\n",
        "            for metric in [f'PCK@{t:.2f}' for t in self.thresholds]:\n",
        "                results['per_category'][cat][metric] = np.mean(metrics[metric])\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _predict_keypoints(\n",
        "        self,\n",
        "        src_img: torch.Tensor,\n",
        "        tgt_img: torch.Tensor,\n",
        "        src_kps: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Predict target keypoints.\"\"\"\n",
        "        src_feat = self.model(src_img)[0]\n",
        "        tgt_feat = self.model(tgt_img)[0]\n",
        "\n",
        "        H_s, W_s, D = src_feat.shape\n",
        "        H_t, W_t, _ = tgt_feat.shape\n",
        "\n",
        "        patch_size = self.model.stride\n",
        "\n",
        "        src_kps_patch = (src_kps / patch_size).long()\n",
        "        src_kps_patch[:, 0] = src_kps_patch[:, 0].clamp(0, W_s - 1)\n",
        "        src_kps_patch[:, 1] = src_kps_patch[:, 1].clamp(0, H_s - 1)\n",
        "\n",
        "        N = src_kps.shape[0]\n",
        "        tgt_kps_pred = torch.zeros(N, 2, device=src_kps.device)\n",
        "\n",
        "        for i in range(N):\n",
        "            x, y = src_kps_patch[i]\n",
        "            src_vec = src_feat[y, x]\n",
        "\n",
        "            similarity = F.cosine_similarity(\n",
        "                src_vec.view(1, 1, 1, D),\n",
        "                tgt_feat.unsqueeze(0),\n",
        "                dim=-1\n",
        "            ).squeeze(0)\n",
        "\n",
        "            max_idx = similarity.flatten().argmax()\n",
        "            pred_y = max_idx // W_t\n",
        "            pred_x = max_idx % W_t\n",
        "\n",
        "            tgt_kps_pred[i, 0] = pred_x * patch_size + patch_size // 2\n",
        "            tgt_kps_pred[i, 1] = pred_y * patch_size + patch_size // 2\n",
        "\n",
        "        return tgt_kps_pred\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "evaluator = FinetunedEvaluator(\n",
        "    model=model,\n",
        "    dataloader=test_loader,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "results = evaluator.evaluate()\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINE-TUNED MODEL RESULTS\")\n",
        "print(\"=\"*70)\n",
        "for metric, vals in results['overall'].items():\n",
        "    print(f\"   {metric}: {vals['mean']:.4f} ± {vals['std']:.4f}\")\n",
        "print(f\"\\n⏱  Inference: {results['inference_time_ms']:.2f} ms/pair\")\n",
        "print(f\" Evaluated: {results['num_pairs']} pairs\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save results\n",
        "results_path = f\"{RESULTS_DIR}/{config.experiment_name}_results.json\"\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"\\n Results saved: {results_path}\")"
      ],
      "metadata": {
        "id": "dD_pFejLN71J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 10: Compare Baseline vs Fine-tuned\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def plot_comparison(baseline_results, finetuned_results):\n",
        "    \"\"\"Plot baseline vs fine-tuned comparison.\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    thresholds = [0.05, 0.10, 0.15, 0.20]\n",
        "\n",
        "    # Extract PCK values\n",
        "    baseline_pck = [baseline_results['overall'][f'PCK@{t:.2f}']['mean']\n",
        "                    for t in thresholds]\n",
        "    finetuned_pck = [finetuned_results['overall'][f'PCK@{t:.2f}']['mean']\n",
        "                     for t in thresholds]\n",
        "\n",
        "    # Plot 1: PCK Curves\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(thresholds, baseline_pck, marker='o', linewidth=2,\n",
        "             label='Baseline (Frozen)', markersize=8, color='#E74C3C')\n",
        "    ax1.plot(thresholds, finetuned_pck, marker='s', linewidth=2,\n",
        "             label=f'Fine-tuned ({config.num_layers_to_finetune} layers)',\n",
        "             markersize=8, color='#27AE60')\n",
        "\n",
        "    ax1.set_xlabel('Threshold', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('PCK', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('PCK Comparison', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim([0, 1])\n",
        "\n",
        "    # Plot 2: Improvement Bar Chart\n",
        "    ax2 = axes[1]\n",
        "    improvements = [(ft - bl) * 100 for bl, ft in zip(baseline_pck, finetuned_pck)]\n",
        "    colors = ['#27AE60' if imp > 0 else '#E74C3C' for imp in improvements]\n",
        "\n",
        "    bars = ax2.bar(range(len(thresholds)), improvements, color=colors, alpha=0.8)\n",
        "    ax2.set_xticks(range(len(thresholds)))\n",
        "    ax2.set_xticklabels([f'PCK@{t:.2f}' for t in thresholds])\n",
        "    ax2.set_ylabel('Improvement (%)', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Fine-tuning Improvement', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
        "\n",
        "    for bar, val in zip(bars, improvements):\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, height,\n",
        "                f'{val:+.2f}%', ha='center',\n",
        "                va='bottom' if height > 0 else 'top',\n",
        "                fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    save_path = f'{RESULTS_DIR}/{config.experiment_name}_comparison.png'\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n Saved: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Load baseline results\n",
        "baseline_path = f'{RESULTS_DIR}/dinov2_vitb14_results.json'\n",
        "with open(baseline_path, 'r') as f:\n",
        "    baseline_results = json.load(f)\n",
        "\n",
        "# Plot comparison\n",
        "plot_comparison(baseline_results, results)\n",
        "\n",
        "# Print summary table\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_data = []\n",
        "for t in [0.05, 0.10, 0.15, 0.20]:\n",
        "    metric = f'PCK@{t:.2f}'\n",
        "    bl = baseline_results['overall'][metric]['mean']\n",
        "    ft = results['overall'][metric]['mean']\n",
        "    imp = (ft - bl) * 100\n",
        "\n",
        "    comparison_data.append({\n",
        "        'Metric': metric,\n",
        "        'Baseline': f'{bl:.4f}',\n",
        "        'Fine-tuned': f'{ft:.4f}',\n",
        "        'Improvement': f'{imp:+.2f}%'\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "MJcot-zJOBAo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}