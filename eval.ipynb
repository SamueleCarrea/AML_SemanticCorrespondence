{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 1: Setup Progetto (usando struttura esistente)\n",
        "# ============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\" AML Semantic Correspondence - Training-Free Baseline\\n\")\n",
        "\n",
        "# 1. Mount Google Drive (se non già montato)\n",
        "if not Path('/content/drive').exists():\n",
        "    drive.mount('/content/drive')\n",
        "    print(\" Google Drive mounted\\n\")\n",
        "else:\n",
        "    print(\" Google Drive already mounted\\n\")\n",
        "\n",
        "# 2. Usa la tua struttura esistente\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/AML'\n",
        "DATA_DIR = f'{PROJECT_ROOT}/dataset' # Reverted to 'dataset' as per original context\n",
        "CHECKPOINT_DIR = f'{PROJECT_ROOT}/checkpoints'\n",
        "RESULTS_DIR = f'{PROJECT_ROOT}/results'\n",
        "\n",
        "# Ensure these directories exist (they will be created inside MyDrive/AML)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# 3. Clone/Copy repository files\n",
        "GITHUB_REPO_URL = 'https://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@github.com/SamueleCarrea/AML_SemanticCorrespondence'\n",
        "LOCAL_REPO_NAME = 'AML_SemanticCorrespondence'\n",
        "\n",
        "if not Path(LOCAL_REPO_NAME).exists():\n",
        "    print(f\"\\n Cloning repository {GITHUB_REPO_URL} into {LOCAL_REPO_NAME}...\")\n",
        "    !git clone {GITHUB_REPO_URL} {LOCAL_REPO_NAME}\n",
        "    print(\" Repository cloned\")\n",
        "else:\n",
        "    print(f\"\\n Repository {LOCAL_REPO_NAME} already exists.\")\n",
        "    # Check if it's a git repo before trying to pull\n",
        "    if Path(LOCAL_REPO_NAME, '.git').exists():\n",
        "        print(\" Pulling latest changes...\")\n",
        "        %cd {LOCAL_REPO_NAME}\n",
        "        !git pull\n",
        "        %cd ..\n",
        "        print(\" Repository updated\")\n",
        "    else:\n",
        "        print(\" Directory exists but is not a Git repository. Skipping pull.\")\n",
        "\n",
        "# Aggiungi al path la directory locale del repository\n",
        "sys.path.insert(0, LOCAL_REPO_NAME)\n",
        "\n",
        "# 4. Verifica GPU\n",
        "import torch\n",
        "print(f\"\\n  GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"\\n Setup complete!\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj-JAohKd36B",
        "outputId": "60990807-1bc2-49b8-e898-ba88112dc294"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AML Semantic Correspondence - Training-Free Baseline\n",
            "\n",
            "Mounted at /content/drive\n",
            " Google Drive mounted\n",
            "\n",
            "\n",
            " Cloning repository https://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@github.com/SamueleCarrea/AML_SemanticCorrespondence into AML_SemanticCorrespondence...\n",
            "Cloning into 'AML_SemanticCorrespondence'...\n",
            "remote: Enumerating objects: 231, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 231 (delta 10), reused 14 (delta 4), pack-reused 204 (from 1)\u001b[K\n",
            "Receiving objects: 100% (231/231), 122.02 KiB | 13.56 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            " Repository cloned\n",
            "\n",
            "  GPU: Tesla T4\n",
            "   VRAM: 15.8 GB\n",
            "\n",
            " Setup complete!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {LOCAL_REPO_NAME}\n",
        "!git pull\n",
        "!git checkout eval\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI4XOuCGdPhb",
        "outputId": "d5982dbd-09aa-45eb-a3ba-ec3429e09362"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AML_SemanticCorrespondence\n",
            "Already up to date.\n",
            "Branch 'eval' set up to track remote branch 'eval' from 'origin'.\n",
            "Switched to a new branch 'eval'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 2: Install Dependencies\n",
        "# ============================================================================\n",
        "\n",
        "print(\" Installing dependencies...\\n\")\n",
        "\n",
        "# Installa da requirements.txt clonato\n",
        "!pip install -q -r {LOCAL_REPO_NAME}/requirements.txt\n",
        "\n",
        "# Verifica veloce\n",
        "import torch\n",
        "print(f\"\\n PyTorch {torch.__version__}\")\n",
        "print(f\" CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "print(\" Dependencies installed!\\n\")"
      ],
      "metadata": {
        "id": "szECarDEfY7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d3eeb0-ec17-489d-e5cf-4e83ec6c9d32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Installing dependencies...\n",
            "\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.4/86.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            " PyTorch 2.9.0+cu126\n",
            " CUDA available: True\n",
            " Dependencies installed!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 3: Load SPair-71k Dataset\n",
        "# ============================================================================\n",
        "\n",
        "from dataset.spair import SPairDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "SPAIR_ROOT = f'{DATA_DIR}/Spair-71k'\n",
        "\n",
        "if not Path(SPAIR_ROOT).exists():\n",
        "    raise FileNotFoundError(f\"Dataset not found: {SPAIR_ROOT}\")\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = SPairDataset(\n",
        "    root=SPAIR_ROOT,\n",
        "    split='test',\n",
        "    size='large',\n",
        "    long_side=518,\n",
        "    normalize=True,\n",
        "    load_segmentation=False\n",
        ")\n",
        "\n",
        "# DataLoader\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "# Sanity check - Mostra tutte le chiavi disponibili\n",
        "sample = test_dataset[0]\n",
        "print(f\" Loaded {len(test_dataset)} pairs from test split (large)\")\n",
        "print(f\" Sample pair: {sample['pair_id']}\")\n",
        "print(f\" Image shapes: src={sample['src_img'].shape}, tgt={sample['tgt_img'].shape}\")\n",
        "print(f\" Keypoints: {len(sample['src_kps'])} correspondences\")\n",
        "print(f\"  Category: {sample['category']}\")\n",
        "print(f\"\\n Available keys in sample:\")\n",
        "for key in sample.keys():\n",
        "    if isinstance(sample[key], torch.Tensor):\n",
        "        print(f\"   - {key}: {sample[key].shape} ({sample[key].dtype})\")\n",
        "    else:\n",
        "        print(f\"   - {key}: {sample[key]}\")"
      ],
      "metadata": {
        "id": "ZyWnOAFugfAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "054eb5ea-f602-4ce0-e591-ea0df8a3ab92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 12234 pairs from test split (large)\n",
            " Loaded 12234 pairs from test split (large)\n",
            " Sample pair: aeroplane:2008_002719-2008_004100\n",
            " Image shapes: src=torch.Size([3, 345, 518]), tgt=torch.Size([3, 344, 518])\n",
            " Keypoints: 3 correspondences\n",
            "  Category: aeroplane\n",
            "\n",
            " Available keys in sample:\n",
            "   - src_img: torch.Size([3, 345, 518]) (torch.float32)\n",
            "   - tgt_img: torch.Size([3, 344, 518]) (torch.float32)\n",
            "   - src_kps: torch.Size([3, 2]) (torch.float32)\n",
            "   - tgt_kps: torch.Size([3, 2]) (torch.float32)\n",
            "   - valid_mask: torch.Size([3]) (torch.bool)\n",
            "   - category: aeroplane\n",
            "   - pair_id: aeroplane:2008_002719-2008_004100\n",
            "   - src_scale: torch.Size([]) (torch.float32)\n",
            "   - tgt_scale: torch.Size([]) (torch.float32)\n",
            "   - src_orig_size: torch.Size([2]) (torch.int64)\n",
            "   - tgt_orig_size: torch.Size([2]) (torch.int64)\n",
            "   - src_bbox: torch.Size([4]) (torch.float32)\n",
            "   - tgt_bbox: torch.Size([4]) (torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 4: Unified Backbone Registry & Factory\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class BackboneConfig:\n",
        "    \"\"\"Configuration for a vision backbone.\"\"\"\n",
        "    name: str\n",
        "    patch_size: int\n",
        "    embed_dim: int\n",
        "    hub_name: str  # Nome per torch.hub o path checkpoint\n",
        "    type: str  # 'dinov2', 'dinov3', 'sam'\n",
        "\n",
        "\n",
        "# Registry di tutti i backbones supportati\n",
        "BACKBONE_REGISTRY = {\n",
        "    # DINOv2 variants\n",
        "    'dinov2_vitb14': BackboneConfig('DINOv2-ViT-B/14', 14, 768, 'dinov2_vitb14', 'dinov2'),\n",
        "    # DINOv3 (quando disponibile, per ora usa DINOv2)\n",
        "    'dinov3_vitb16': BackboneConfig('DINOv3-ViT-B/16', 16, 768, 'dinov3_vitb16', 'dinov3'),\n",
        "    # SAM variants (opzionale)\n",
        "    'sam_vit_b': BackboneConfig('SAM-ViT-B', 16, 768, 'vit_b', 'sam'),\n",
        "}\n",
        "\n",
        "\n",
        "class UnifiedBackbone(nn.Module):\n",
        "    \"\"\"Unified interface per tutti i backbones.\"\"\"\n",
        "\n",
        "    def __init__(self, backbone_name: str, device: str = 'cuda'):\n",
        "        super().__init__()\n",
        "\n",
        "        if backbone_name not in BACKBONE_REGISTRY:\n",
        "            raise ValueError(f\"Unknown backbone: {backbone_name}. Available: {list(BACKBONE_REGISTRY.keys())}\")\n",
        "\n",
        "        self.config = BACKBONE_REGISTRY[backbone_name]\n",
        "        self.device = device\n",
        "\n",
        "        print(f\" Loading {self.config.name}...\")\n",
        "\n",
        "        # Load model based on type\n",
        "        if self.config.type == 'dinov2':\n",
        "            self.model = self._load_dinov2()\n",
        "        elif self.config.type == 'dinov3':\n",
        "            self.model = self._load_dinov3()\n",
        "        elif self.config.type == 'sam':\n",
        "            self.model = self._load_sam()\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Type {self.config.type} not implemented\")\n",
        "\n",
        "        self.model.eval()\n",
        "        self.model.to(device)\n",
        "\n",
        "        print(f\" {self.config.name} loaded\")\n",
        "        print(f\"   Patch size: {self.config.patch_size}\")\n",
        "        print(f\"   Embedding: {self.config.embed_dim}D\\n\")\n",
        "\n",
        "    def _load_dinov2(self):\n",
        "        \"\"\"Load DINOv2 from torch.hub.\"\"\"\n",
        "\n",
        "        return torch.hub.load( 'facebookresearch/dinov2', self.config.hub_name,pretrained=True,verbose=False)\n",
        "\n",
        "    def _load_dinov3(self):\n",
        "        \"\"\"Load DINOv3 from torch.hub.\"\"\"\n",
        "\n",
        "        return torch.hub.load('facebookresearch/dinov3', self.config.hub_name,pretrained=True,verbose=False)\n",
        "\n",
        "\n",
        "    def _load_sam(self):\n",
        "        \"\"\"Load SAM (placeholder - requires segment-anything package).\"\"\"\n",
        "        try:\n",
        "            from segment_anything import sam_model_registry\n",
        "            checkpoint_path = f'/tmp/sam_{self.config.hub_name}.pth'\n",
        "\n",
        "            # Download if needed\n",
        "            if not Path(checkpoint_path).exists():\n",
        "                urls = {\n",
        "                    'vit_b': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth',\n",
        "                }\n",
        "                torch.hub.download_url_to_file(urls[self.config.hub_name], checkpoint_path)\n",
        "\n",
        "            return sam_model_registry[self.config.hub_name](checkpoint=checkpoint_path)\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"  SAM requires: pip install git+https://github.com/facebookresearch/segment-anything.git\")\n",
        "            raise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def extract_features(self, image: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Extract dense features.\n",
        "\n",
        "        Args:\n",
        "            image: (B, 3, H, W)\n",
        "        Returns:\n",
        "            features: (B, H_patches, W_patches, D)\n",
        "        \"\"\"\n",
        "        B, C, H, W = image.shape\n",
        "        image = image.to(self.device)\n",
        "\n",
        "        if self.config.type == 'dinov2' or self.config.type == 'dinov3':\n",
        "            # DINOv2 extraction\n",
        "            output = self.model.forward_features(image)\n",
        "            patch_tokens = output['x_norm_patchtokens']  # (B, N, D)\n",
        "\n",
        "            H_p = H // self.config.patch_size\n",
        "            W_p = W // self.config.patch_size\n",
        "\n",
        "            features = patch_tokens.view(B, H_p, W_p, self.config.embed_dim)\n",
        "\n",
        "        elif self.config.type == 'sam':\n",
        "            # SAM extraction\n",
        "            features = self.model.image_encoder(\n",
        "                F.interpolate(image, size=(1024, 1024), mode='bilinear')\n",
        "            )\n",
        "            # Resize back and permute\n",
        "            H_p = H // self.config.patch_size\n",
        "            W_p = W // self.config.patch_size\n",
        "            features = F.interpolate(features, size=(H_p, W_p), mode='bilinear')\n",
        "            features = features.permute(0, 2, 3, 1)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "# Test registry\n",
        "print(\" Available Backbones:\")\n",
        "print(\"=\" * 60)\n",
        "for name, config in BACKBONE_REGISTRY.items():\n",
        "    print(f\"   {name:20s} \\u2192 {config.name}\")\n",
        "print(\"=\" * 60)\n",
        "print()"
      ],
      "metadata": {
        "id": "DQfunoH8gjHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48bfbb6-20c1-4030-80be-c5e4ab9f2493"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Available Backbones:\n",
            "============================================================\n",
            "   dinov2_vitb14        → DINOv2-ViT-B/14\n",
            "   dinov3_vitb16        → DINOv3-ViT-B/16\n",
            "   sam_vit_b            → SAM-ViT-B\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 5: Unified Correspondence Matcher\n",
        "# ============================================================================\n",
        "\n",
        "class CorrespondenceMatcher:\n",
        "    \"\"\"Semantic correspondence matcher (training-free baseline).\"\"\"\n",
        "\n",
        "    def __init__(self, backbone: UnifiedBackbone):\n",
        "        self.backbone = backbone\n",
        "        self.device = backbone.device\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def match(\n",
        "        self,\n",
        "        src_img: torch.Tensor,\n",
        "        tgt_img: torch.Tensor,\n",
        "        src_kps: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Find correspondences for source keypoints.\n",
        "\n",
        "        Args:\n",
        "            src_img: (1, 3, H, W)\n",
        "            tgt_img: (1, 3, H, W)\n",
        "            src_kps: (N, 2) in pixel coords (x, y)\n",
        "\n",
        "        Returns:\n",
        "            tgt_kps_pred: (N, 2) predicted target keypoints\n",
        "        \"\"\"\n",
        "        # Extract features\n",
        "        src_feat = self.backbone.extract_features(src_img)[0]  # (H_s, W_s, D)\n",
        "        tgt_feat = self.backbone.extract_features(tgt_img)[0]  # (H_t, W_t, D)\n",
        "\n",
        "        H_s, W_s, D = src_feat.shape\n",
        "        H_t, W_t, _ = tgt_feat.shape\n",
        "\n",
        "        patch_size = self.backbone.config.patch_size\n",
        "\n",
        "        # Convert keypoint coords to patch indices\n",
        "        src_kps_patch = (src_kps / patch_size).long()\n",
        "        src_kps_patch[:, 0] = src_kps_patch[:, 0].clamp(0, W_s - 1)\n",
        "        src_kps_patch[:, 1] = src_kps_patch[:, 1].clamp(0, H_s - 1)\n",
        "\n",
        "        # Match each keypoint\n",
        "        N = src_kps.shape[0]\n",
        "        tgt_kps_pred = torch.zeros(N, 2, device=src_kps.device)\n",
        "\n",
        "        for i in range(N):\n",
        "            x, y = src_kps_patch[i]\n",
        "            src_vec = src_feat[y, x]  # (D,)\n",
        "\n",
        "            # Cosine similarity\n",
        "            similarity = F.cosine_similarity(\n",
        "                src_vec.view(1, 1, 1, D),\n",
        "                tgt_feat.unsqueeze(0),\n",
        "                dim=-1\n",
        "            ).squeeze(0)  # (H_t, W_t)\n",
        "\n",
        "            # Argmax\n",
        "            max_idx = similarity.flatten().argmax()\n",
        "            pred_y = max_idx // W_t\n",
        "            pred_x = max_idx % W_t\n",
        "\n",
        "            # Convert back to pixels\n",
        "            tgt_kps_pred[i, 0] = pred_x * patch_size + patch_size // 2\n",
        "            tgt_kps_pred[i, 1] = pred_y * patch_size + patch_size // 2\n",
        "\n",
        "        return tgt_kps_pred\n",
        "\n",
        "\n",
        "print(\" Unified matcher ready!\")"
      ],
      "metadata": {
        "id": "PlF_Vgmngt2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be661b8-62ac-4f22-8725-1a1db55a3ffe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Unified matcher ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 6: PCK Metrics Implementation\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "\n",
        "def compute_pck(\n",
        "    pred_kps: torch.Tensor,\n",
        "    gt_kps: torch.Tensor,\n",
        "    image_size: tuple,\n",
        "    thresholds: List[float] = [0.05, 0.10, 0.15, 0.20]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute Percentage of Correct Keypoints (PCK) at multiple thresholds.\n",
        "\n",
        "    Args:\n",
        "        pred_kps: (N, 2) predicted keypoints in (x, y) format\n",
        "        gt_kps: (N, 2) ground truth keypoints in (x, y) format\n",
        "        image_size: (H, W) image dimensions\n",
        "        thresholds: List of normalized distance thresholds\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with PCK@T for each threshold\n",
        "    \"\"\"\n",
        "    H, W = image_size\n",
        "\n",
        "    # Use max dimension for normalization (standard in SPair-71k)\n",
        "    max_dim = max(H, W)\n",
        "    # =======================could be error to calculate euclidian distance =======================\n",
        "    # Compute Euclidean distance\n",
        "    distances = torch.norm(pred_kps - gt_kps, dim=1)  # (N,)\n",
        "\n",
        "    # Normalize by image size\n",
        "    normalized_distances = distances / max_dim\n",
        "\n",
        "    # Compute PCK for each threshold\n",
        "    results = {}\n",
        "    for t in thresholds:\n",
        "        correct = (normalized_distances <= t).float()\n",
        "        pck = correct.mean().item()\n",
        "        results[f'PCK@{t:.2f}'] = pck\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def compute_pck_per_keypoint(\n",
        "    pred_kps: torch.Tensor,\n",
        "    gt_kps: torch.Tensor,\n",
        "    image_size: tuple,\n",
        "    thresholds: List[float] = [0.05, 0.10, 0.15, 0.20]\n",
        ") -> Dict[int, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Compute PCK per individual keypoint.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping keypoint_id -> {PCK@T metrics}\n",
        "    \"\"\"\n",
        "    H, W = image_size\n",
        "    max_dim = max(H, W)\n",
        "\n",
        "    N = pred_kps.shape[0]\n",
        "    distances = torch.norm(pred_kps - gt_kps, dim=1) / max_dim\n",
        "\n",
        "    results = {}\n",
        "    for i in range(N):\n",
        "        kp_results = {}\n",
        "        for t in thresholds:\n",
        "            correct = (distances[i] <= t).float().item()\n",
        "            kp_results[f'PCK@{t:.2f}'] = correct\n",
        "        results[i] = kp_results\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Test metrics\n",
        "print(\" Testing PCK metrics...\\n\")\n",
        "\n",
        "# Dummy data\n",
        "pred = torch.tensor([[100.0, 150.0], [200.0, 250.0], [50.0, 75.0]])\n",
        "gt = torch.tensor([[105.0, 155.0], [195.0, 245.0], [48.0, 72.0]])\n",
        "img_size = (480, 640)\n",
        "\n",
        "pck_results = compute_pck(pred, gt, img_size)\n",
        "\n",
        "print(\" PCK Results (overall):\")\n",
        "for metric, value in pck_results.items():\n",
        "    print(f\"   {metric}: {value:.4f} ({value*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n Metrics implementation working!\")"
      ],
      "metadata": {
        "id": "4f9KkzVFg6I1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f52c7adc-cae6-4ae1-bdd6-68e2bd1cc3c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Testing PCK metrics...\n",
            "\n",
            " PCK Results (overall):\n",
            "   PCK@0.05: 1.0000 (100.00%)\n",
            "   PCK@0.10: 1.0000 (100.00%)\n",
            "   PCK@0.15: 1.0000 (100.00%)\n",
            "   PCK@0.20: 1.0000 (100.00%)\n",
            "\n",
            " Metrics implementation working!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 7: Unified Evaluation Engine\n",
        "# ============================================================================\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "class UnifiedEvaluator:\n",
        "    \"\"\"Evaluation engine per tutti i backbones.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataloader,\n",
        "        device: str = 'cuda',\n",
        "        thresholds: list = [0.05, 0.10, 0.15, 0.20]\n",
        "    ):\n",
        "        self.dataloader = dataloader\n",
        "        self.device = device\n",
        "        self.thresholds = thresholds\n",
        "        self.results = {}\n",
        "\n",
        "    def evaluate_backbone(\n",
        "        self,\n",
        "        backbone_name: str,\n",
        "        num_samples: int = None\n",
        "    ) -> dict:\n",
        "        \"\"\"\n",
        "        Evaluate a single backbone.\n",
        "\n",
        "        Args:\n",
        "            backbone_name: Name from BACKBONE_REGISTRY\n",
        "            num_samples: Max number of samples (None = all)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"EVALUATING: {BACKBONE_REGISTRY[backbone_name].name}\")\n",
        "        print('='*70)\n",
        "\n",
        "        # Initialize\n",
        "        backbone = UnifiedBackbone(backbone_name, device=self.device)\n",
        "        matcher = CorrespondenceMatcher(backbone)\n",
        "\n",
        "        # Storage\n",
        "        all_pck = defaultdict(list)\n",
        "        per_category = defaultdict(lambda: defaultdict(list))\n",
        "        inference_times = []\n",
        "\n",
        "        # Evaluation loop\n",
        "        n_processed = 0\n",
        "        pbar = tqdm(self.dataloader, desc=f\"{backbone_name}\")\n",
        "\n",
        "        for batch in pbar:\n",
        "            if num_samples and n_processed >= num_samples:\n",
        "                break\n",
        "\n",
        "            # Extract data\n",
        "            src_img = batch['src_img'].to(self.device)\n",
        "            tgt_img = batch['tgt_img'].to(self.device)\n",
        "            src_kps = batch['src_kps'][0]  # (N, 2)\n",
        "            tgt_kps = batch['tgt_kps'][0]\n",
        "            valid_mask = batch['valid_mask'][0]\n",
        "            category = batch['category'][0]\n",
        "\n",
        "            # Filter valid keypoints\n",
        "            src_kps_valid = src_kps[valid_mask]\n",
        "            tgt_kps_valid = tgt_kps[valid_mask]\n",
        "\n",
        "            if len(src_kps_valid) == 0:\n",
        "                continue\n",
        "\n",
        "            # Predict (with timing)\n",
        "            start = time.time()\n",
        "            tgt_kps_pred = matcher.match(src_img, tgt_img, src_kps_valid)\n",
        "            inference_times.append(time.time() - start)\n",
        "\n",
        "            # Compute metrics\n",
        "            H, W = tgt_img.shape[2:]\n",
        "            pck_scores = compute_pck(tgt_kps_pred, tgt_kps_valid, (H, W), self.thresholds)\n",
        "\n",
        "            # Store\n",
        "            for metric, value in pck_scores.items():\n",
        "                all_pck[metric].append(value)\n",
        "                per_category[category][metric].append(value)\n",
        "\n",
        "            n_processed += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            if len(all_pck['PCK@0.10']) > 0:\n",
        "                avg_pck = np.mean(all_pck['PCK@0.10'])\n",
        "                pbar.set_postfix({'PCK@0.10': f'{avg_pck:.4f}'})\n",
        "\n",
        "        # Aggregate results\n",
        "        results = self._aggregate_results(\n",
        "            backbone_name, all_pck, per_category, inference_times, n_processed\n",
        "        )\n",
        "\n",
        "        self.results[backbone_name] = results\n",
        "        self._print_summary(results)\n",
        "\n",
        "        # Cleanup\n",
        "        del backbone, matcher\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _aggregate_results(self, backbone_name, all_pck, per_category, times, n_pairs):\n",
        "        \"\"\"Aggregate all metrics.\"\"\"\n",
        "\n",
        "        results = {\n",
        "            'backbone': backbone_name,\n",
        "            'display_name': BACKBONE_REGISTRY[backbone_name].name,\n",
        "            'num_pairs': n_pairs,\n",
        "            'inference_time_ms': np.mean(times) * 1000,\n",
        "            'overall': {},\n",
        "            'per_category': {}\n",
        "        }\n",
        "\n",
        "        # Overall\n",
        "        for metric in [f'PCK@{t:.2f}' for t in self.thresholds]:\n",
        "            values = all_pck[metric]\n",
        "            results['overall'][metric] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "            }\n",
        "\n",
        "        # Per-category\n",
        "        for cat, metrics in per_category.items():\n",
        "            results['per_category'][cat] = {}\n",
        "            for metric in [f'PCK@{t:.2f}' for t in self.thresholds]:\n",
        "                results['per_category'][cat][metric] = np.mean(metrics[metric])\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _print_summary(self, results):\n",
        "        \"\"\"Print evaluation summary.\"\"\"\n",
        "\n",
        "        print(f\"\\n {results['display_name']} Results:\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        for metric, values in results['overall'].items():\n",
        "            print(f\"   {metric}: {values['mean']:.4f} ± {values['std']:.4f}\")\n",
        "\n",
        "        print(f\"\\n⏱  Avg inference time: {results['inference_time_ms']:.2f} ms/pair\")\n",
        "        print(f\" Evaluated on {results['num_pairs']} pairs\")\n",
        "\n",
        "    def compare_all(self) -> pd.DataFrame:\n",
        "        \"\"\"Create comparison table.\"\"\"\n",
        "\n",
        "        rows = []\n",
        "        for name, res in self.results.items():\n",
        "            row = {\n",
        "                'Backbone': res['display_name'],\n",
        "                'Pairs': res['num_pairs'],\n",
        "                'Time (ms)': f\"{res['inference_time_ms']:.1f}\",\n",
        "            }\n",
        "\n",
        "            for metric, vals in res['overall'].items():\n",
        "                row[metric] = f\"{vals['mean']:.4f}\"\n",
        "\n",
        "            rows.append(row)\n",
        "\n",
        "        df = pd.DataFrame(rows)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FINAL COMPARISON\")\n",
        "        print(\"=\"*70)\n",
        "        print(df.to_string(index=False))\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "#Device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = UnifiedEvaluator(\n",
        "    dataloader=test_loader,\n",
        "    device=device,\n",
        "    thresholds=[0.05, 0.10, 0.15, 0.20]\n",
        ")\n",
        "\n",
        "print(\"\\n Unified evaluator ready!\")"
      ],
      "metadata": {
        "id": "z74I72F_g-MG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f71c921-644c-426b-e3b2-02c1f211bd06"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            " Unified evaluator ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 8A: Evaluate DINOv2-ViT-B/14\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "BACKBONE_NAME = 'dinov2_vitb14'\n",
        "USE_SUBSET = True\n",
        "NUM_SAMPLES = 100 if USE_SUBSET else None\n",
        "\n",
        "print(f\" Evaluating: {BACKBONE_REGISTRY[BACKBONE_NAME].name}\")\n",
        "print(f\"   Samples: {NUM_SAMPLES if NUM_SAMPLES else 'ALL (1814)'}\")\n",
        "print(f\"   Device: {device}\\n\")\n",
        "\n",
        "# Evaluate\n",
        "try:\n",
        "    results_dinov2 = evaluator.evaluate_backbone(BACKBONE_NAME, num_samples=NUM_SAMPLES)\n",
        "\n",
        "    # Save results immediately\n",
        "    output_file = f'{RESULTS_DIR}/dinov2_vitb14_results.json'\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results_dinov2, f, indent=2)\n",
        "\n",
        "    print(f\"\\n Results saved: {output_file}\")\n",
        "\n",
        "    # Show summary\n",
        "    print(f\"\\n DINOv2 Summary:\")\n",
        "    print(f\"   PCK@0.10: {results_dinov2['overall']['PCK@0.10']['mean']:.4f}\")\n",
        "    print(f\"   Time: {results_dinov2['inference_time_ms']:.2f} ms/pair\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n DINOv2 evaluation complete!\")"
      ],
      "metadata": {
        "id": "JWdsxbkahCHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66df5084-45b2-43d4-b3d6-879ad5adde56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Evaluating: DINOv2-ViT-B/14\n",
            "   Samples: 100\n",
            "   Device: cuda\n",
            "\n",
            "\n",
            "======================================================================\n",
            "EVALUATING: DINOv2-ViT-B/14\n",
            "======================================================================\n",
            " Loading DINOv2-ViT-B/14...\n",
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitb14_pretrain.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 330M/330M [00:02<00:00, 135MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " DINOv2-ViT-B/14 loaded\n",
            "   Patch size: 14\n",
            "   Embedding: 768D\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "dinov2_vitb14:   0%|          | 0/12234 [00:02<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Error: Input image height 345 is not a multiple of patch height 14\n",
            "\n",
            " DINOv2 evaluation complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-4207710299.py\", line 19, in <cell line: 0>\n",
            "    results_dinov2 = evaluator.evaluate_backbone(BACKBONE_NAME, num_samples=NUM_SAMPLES)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3280199817.py\", line 74, in evaluate_backbone\n",
            "    tgt_kps_pred = matcher.match(src_img, tgt_img, src_kps_valid)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1526860766.py\", line 31, in match\n",
            "    src_feat = self.backbone.extract_features(src_img)[0]  # (H_s, W_s, D)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2890554399.py\", line 108, in extract_features\n",
            "    output = self.model.forward_features(image)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py\", line 261, in forward_features\n",
            "    x = self.prepare_tokens_with_masks(x, masks)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py\", line 218, in prepare_tokens_with_masks\n",
            "    x = self.patch_embed(x)\n",
            "        ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py\", line 72, in forward\n",
            "    assert H % patch_H == 0, f\"Input image height {H} is not a multiple of patch height {patch_H}\"\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "AssertionError: Input image height 345 is not a multiple of patch height 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 8B: Evaluate DINOv3-ViT-B/14 (con fallback a DINOv2)\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "\n",
        "# Configuration\n",
        "BACKBONE_NAME = 'dinov3_vitb14'\n",
        "USE_SUBSET = True\n",
        "NUM_SAMPLES = 100 if USE_SUBSET else None\n",
        "\n",
        "print(f\" Evaluating: {BACKBONE_REGISTRY[BACKBONE_NAME].name}\")\n",
        "print(f\"   Samples: {NUM_SAMPLES if NUM_SAMPLES else 'ALL (1814)'}\")\n",
        "print(f\"   Device: {device}\\n\")\n",
        "\n",
        "# Evaluate\n",
        "try:\n",
        "    results_dinov3 = evaluator.evaluate_backbone(BACKBONE_NAME, num_samples=NUM_SAMPLES)\n",
        "\n",
        "    # Save results\n",
        "    output_file = f'{RESULTS_DIR}/dinov3_vitb14_results.json'\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results_dinov3, f, indent=2)\n",
        "\n",
        "    print(f\"\\n Results saved: {output_file}\")\n",
        "\n",
        "    # Show summary\n",
        "    print(f\"\\n DINOv3 Summary:\")\n",
        "    print(f\"   PCK@0.10: {results_dinov3['overall']['PCK@0.10']['mean']:.4f}\")\n",
        "    print(f\"   Time: {results_dinov3['inference_time_ms']:.2f} ms/pair\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n DINOv3 evaluation complete!\")"
      ],
      "metadata": {
        "id": "yFkXauvhADVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 8C: Evaluate SAM-ViT-B (OPZIONALE)\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "\n",
        "# NOTA: Richiede installazione:\n",
        "# !pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "\n",
        "# Configuration\n",
        "BACKBONE_NAME = 'sam_vit_b'\n",
        "USE_SUBSET = True\n",
        "NUM_SAMPLES = 100 if USE_SUBSET else None\n",
        "\n",
        "print(f\" Evaluating: {BACKBONE_REGISTRY[BACKBONE_NAME].name}\")\n",
        "print(f\"   Samples: {NUM_SAMPLES if NUM_SAMPLES else 'ALL (1814)'}\")\n",
        "print(f\"   Device: {device}\\n\")\n",
        "\n",
        "# Check if SAM is installed\n",
        "try:\n",
        "    import segment_anything\n",
        "    print(\" segment-anything package found\\n\")\n",
        "except ImportError:\n",
        "    print(\" SAM not installed. Installing now...\")\n",
        "    !pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "    print(\" Installation complete\\n\")\n",
        "\n",
        "# Evaluate\n",
        "try:\n",
        "    results_sam = evaluator.evaluate_backbone(BACKBONE_NAME, num_samples=NUM_SAMPLES)\n",
        "\n",
        "    # Save results\n",
        "    output_file = f'{RESULTS_DIR}/sam_vit_b_results.json'\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results_sam, f, indent=2)\n",
        "\n",
        "    print(f\"\\n Results saved: {output_file}\")\n",
        "\n",
        "    # Show summary\n",
        "    print(f\"\\n SAM Summary:\")\n",
        "    print(f\"   PCK@0.10: {results_sam['overall']['PCK@0.10']['mean']:.4f}\")\n",
        "    print(f\"   Time: {results_sam['inference_time_ms']:.2f} ms/pair\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Error: {e}\")\n",
        "    print(\" Tip: SAM might not work well for semantic correspondence\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n SAM evaluation complete (or skipped)!\")"
      ],
      "metadata": {
        "id": "s51DfS-dAIqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 9: Load All Results & Final Comparison\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "print(\" Loading all saved results...\\n\")\n",
        "\n",
        "# Load all saved results\n",
        "all_results = {}\n",
        "\n",
        "result_files = {\n",
        "    'dinov2_vitb14': f'{RESULTS_DIR}/dinov2_vitb14_results.json',\n",
        "    'dinov3_vitb14': f'{RESULTS_DIR}/dinov3_vitb14_results.json',\n",
        "    'sam_vit_b': f'{RESULTS_DIR}/sam_vit_b_results.json',\n",
        "}\n",
        "\n",
        "for backbone_name, filepath in result_files.items():\n",
        "    if Path(filepath).exists():\n",
        "        with open(filepath, 'r') as f:\n",
        "            all_results[backbone_name] = json.load(f)\n",
        "        print(f\" Loaded: {backbone_name}\")\n",
        "    else:\n",
        "        print(f\"  Not found: {filepath}\")\n",
        "\n",
        "print(f\"\\n Loaded {len(all_results)} backbone results\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# Create Comparison Table\n",
        "# ============================================================================\n",
        "\n",
        "def create_comparison_table(results_dict):\n",
        "    \"\"\"Create comparison DataFrame.\"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for name, res in results_dict.items():\n",
        "        row = {\n",
        "            'Backbone': res['display_name'],\n",
        "            'Pairs': res['num_pairs'],\n",
        "            'Time (ms)': f\"{res['inference_time_ms']:.1f}\",\n",
        "        }\n",
        "\n",
        "        for metric, vals in res['overall'].items():\n",
        "            row[metric] = f\"{vals['mean']:.4f}\"\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "comparison_df = create_comparison_table(all_results)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FINAL COMPARISON TABLE\")\n",
        "print(\"=\" * 80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save CSV\n",
        "csv_file = f'{RESULTS_DIR}/final_comparison.csv'\n",
        "comparison_df.to_csv(csv_file, index=False)\n",
        "print(f\"\\n Saved: {csv_file}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Visualizations\n",
        "# ============================================================================\n",
        "\n",
        "def plot_comparison(results_dict):\n",
        "    \"\"\"Generate comparison plots.\"\"\"\n",
        "\n",
        "    if len(results_dict) == 0:\n",
        "        print(\"  No results to plot\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    backbones = list(results_dict.keys())\n",
        "    display_names = [results_dict[b]['display_name'] for b in backbones]\n",
        "    thresholds = [0.05, 0.10, 0.15, 0.20]\n",
        "    colors = sns.color_palette('husl', len(backbones))\n",
        "\n",
        "    # Plot 1: PCK Curves\n",
        "    ax1 = axes[0]\n",
        "    for backbone, color in zip(backbones, colors):\n",
        "        res = results_dict[backbone]\n",
        "        pck_vals = [res['overall'][f'PCK@{t:.2f}']['mean'] for t in thresholds]\n",
        "        ax1.plot(thresholds, pck_vals, marker='o', linewidth=2,\n",
        "                label=res['display_name'], markersize=8, color=color)\n",
        "\n",
        "    ax1.set_xlabel('Threshold', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('PCK', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('PCK Curves', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim([0, 1])\n",
        "\n",
        "    # Plot 2: PCK@0.10 Bar Chart\n",
        "    ax2 = axes[1]\n",
        "    pck_010 = [results_dict[b]['overall']['PCK@0.10']['mean'] for b in backbones]\n",
        "\n",
        "    bars = ax2.bar(range(len(backbones)), pck_010, color=colors, alpha=0.8)\n",
        "    ax2.set_xticks(range(len(backbones)))\n",
        "    ax2.set_xticklabels(display_names, rotation=45, ha='right')\n",
        "    ax2.set_ylabel('PCK@0.10', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylim([0, 1])\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bar, val in zip(bars, pck_010):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{val:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Plot 3: Inference Time\n",
        "    ax3 = axes[2]\n",
        "    times = [results_dict[b]['inference_time_ms'] for b in backbones]\n",
        "    bars = ax3.bar(range(len(backbones)), times, color=colors, alpha=0.8)\n",
        "    ax3.set_xticks(range(len(backbones)))\n",
        "    ax3.set_xticklabels(display_names, rotation=45, ha='right')\n",
        "    ax3.set_ylabel('Time (ms)', fontsize=12, fontweight='bold')\n",
        "    ax3.set_title('Inference Speed', fontsize=14, fontweight='bold')\n",
        "    ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bar, val in zip(bars, times):\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.02,\n",
        "                f'{val:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save\n",
        "    save_path = f'{FIGURES_DIR}/final_comparison.png'\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n Saved: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots\n",
        "plot_comparison(all_results)\n",
        "\n",
        "print(\"\\n Comparison complete!\")"
      ],
      "metadata": {
        "id": "lumBUtEjrtRu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}