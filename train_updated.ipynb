{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNR4LpdBLrhl"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 1: Setup Progetto (riutilizza da eval.ipynb)\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\" AML Semantic Correspondence - Fine-tuning Stage\\n\")\n",
    "\n",
    "# 1. Mount Google Drive\n",
    "if not Path('/content/drive').exists():\n",
    "    drive.mount('/content/drive')\n",
    "    print(\" Google Drive mounted\\n\")\n",
    "else:\n",
    "    print(\" Google Drive already mounted\\n\")\n",
    "\n",
    "# 2. Setup directories\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/AML'\n",
    "DATA_DIR = f'{PROJECT_ROOT}/dataset'\n",
    "CHECKPOINT_DIR = f'{PROJECT_ROOT}/checkpoints'\n",
    "RESULTS_DIR = f'{PROJECT_ROOT}/results'\n",
    "FINETUNED_DIR = f'{PROJECT_ROOT}/finetuned_models'\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(FINETUNED_DIR, exist_ok=True)\n",
    "\n",
    "# 3. Clone repository\n",
    "GITHUB_REPO_URL = 'https://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@github.com/SamueleCarrea/AML_SemanticCorrespondence'\n",
    "LOCAL_REPO_NAME = 'AML_SemanticCorrespondence'\n",
    "\n",
    "if not Path(LOCAL_REPO_NAME).exists():\n",
    "    print(f\"\\n Cloning repository...\")\n",
    "    !git clone {GITHUB_REPO_URL} {LOCAL_REPO_NAME}\n",
    "    print(\" Repository cloned\")\n",
    "else:\n",
    "    print(f\"\\n Repository {LOCAL_REPO_NAME} already exists.\")\n",
    "    if Path(LOCAL_REPO_NAME, '.git').exists():\n",
    "        print(\" Pulling latest changes...\")\n",
    "        %cd {LOCAL_REPO_NAME}\n",
    "        !git pull\n",
    "        %cd ..\n",
    "        print(\" Repository updated\")\n",
    "\n",
    "sys.path.insert(0, LOCAL_REPO_NAME)\n",
    "\n",
    "# 4. GPU info\n",
    "import torch\n",
    "print(f\"\\n  GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\" VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"\\n Setup complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv5vl0bTNhv1"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 2: Install Dependencies\n",
    "# ============================================================================\n",
    "\n",
    "print(\" Installing dependencies...\\n\")\n",
    "\n",
    "!pip install -q -r {LOCAL_REPO_NAME}/requirements.txt\n",
    "!pip install -q tensorboard\n",
    "\n",
    "import torch\n",
    "print(f\"\\n PyTorch {torch.__version__}\")\n",
    "print(f\" CUDA available: {torch.cuda.is_available()}\")\n",
    "print(\"\\n Dependencies installed!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def spair_pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Collate function per SPair-71K:\n",
    "    - pad immagini solo in basso e a destra\n",
    "    - NON altera le coordinate dei keypoints\n",
    "    - permette batch_size > 1\n",
    "    \"\"\"\n",
    "    imgsA, imgsB = [], []\n",
    "    kpsA, kpsB = [], []\n",
    "    meta = []\n",
    "\n",
    "    max_hA = max(item[0].shape[1] for item in batch)\n",
    "    max_wA = max(item[0].shape[2] for item in batch)\n",
    "    max_hB = max(item[1].shape[1] for item in batch)\n",
    "    max_wB = max(item[1].shape[2] for item in batch)\n",
    "\n",
    "    for imgA, imgB, kpA, kpB, m in batch:\n",
    "        padA = (0, max_wA - imgA.shape[2], 0, max_hA - imgA.shape[1])\n",
    "        padB = (0, max_wB - imgB.shape[2], 0, max_hB - imgB.shape[1])\n",
    "\n",
    "        imgsA.append(F.pad(imgA, padA))\n",
    "        imgsB.append(F.pad(imgB, padB))\n",
    "        kpsA.append(kpA)\n",
    "        kpsB.append(kpB)\n",
    "        meta.append(m)\n",
    "\n",
    "    return (\n",
    "        torch.stack(imgsA, dim=0),\n",
    "        torch.stack(imgsB, dim=0),\n",
    "        torch.stack(kpsA, dim=0),\n",
    "        torch.stack(kpsB, dim=0),\n",
    "        meta,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_o7DroGNil-"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 3: Load Datasets (Train, Val, Test)\n",
    "# ============================================================================\n",
    "\n",
    "from dataset.spair import SPairDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "SPAIR_ROOT = f'{DATA_DIR}/Spair-71k'\n",
    "\n",
    "# Load all splits\n",
    "train_dataset = SPairDataset(\n",
    "    root=SPAIR_ROOT,\n",
    "    split='train',\n",
    "    size='large',\n",
    "    long_side=518,\n",
    "    normalize=True,\n",
    "    load_segmentation=False\n",
    ")\n",
    "\n",
    "val_dataset = SPairDataset(\n",
    "    root=SPAIR_ROOT,\n",
    "    split='val',\n",
    "    size='large',\n",
    "    long_side=518,\n",
    "    normalize=True,\n",
    "    load_segmentation=False\n",
    ")\n",
    "\n",
    "test_dataset = SPairDataset(\n",
    "    root=SPAIR_ROOT,\n",
    "    split='test',\n",
    "    size='large',\n",
    "    long_side=518,\n",
    "    normalize=True,\n",
    "    load_segmentation=False\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=spair_pad_collate\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\" Dataset Statistics:\")\n",
    "print(f\"   Train: {len(train_dataset)} pairs\")\n",
    "print(f\"   Val:   {len(val_dataset)} pairs\")\n",
    "print(f\"   Test:  {len(test_dataset)} pairs\")\n",
    "print(f\"\\n Datasets loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Collate function for SPair-71K\n",
    "# - Pads images in the batch to the max H/W (bottom/right padding only)\n",
    "# - Pads keypoints to the max number of keypoints in the batch\n",
    "# - Produces a padded valid_mask so downstream code can ignore padded entries\n",
    "#   (padding does not shift origin, so keypoints stay correct)\n",
    "# =========================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def spair_pad_collate(batch):\n",
    "    \"\"\"Batch is a list of dicts returned by SPairDataset.\"\"\"\n",
    "    # 1) Pad images\n",
    "    max_h_src = max(item['src_img'].shape[1] for item in batch)\n",
    "    max_w_src = max(item['src_img'].shape[2] for item in batch)\n",
    "    max_h_tgt = max(item['tgt_img'].shape[1] for item in batch)\n",
    "    max_w_tgt = max(item['tgt_img'].shape[2] for item in batch)\n",
    "\n",
    "    src_imgs, tgt_imgs = [], []\n",
    "    for item in batch:\n",
    "        src = item['src_img']\n",
    "        tgt = item['tgt_img']\n",
    "        pad_src = (0, max_w_src - src.shape[2], 0, max_h_src - src.shape[1])\n",
    "        pad_tgt = (0, max_w_tgt - tgt.shape[2], 0, max_h_tgt - tgt.shape[1])\n",
    "        src_imgs.append(F.pad(src, pad_src, value=0.0))\n",
    "        tgt_imgs.append(F.pad(tgt, pad_tgt, value=0.0))\n",
    "\n",
    "    out = {\n",
    "        'src_img': torch.stack(src_imgs, dim=0),\n",
    "        'tgt_img': torch.stack(tgt_imgs, dim=0),\n",
    "    }\n",
    "\n",
    "    # 2) Pad keypoints + valid masks\n",
    "    # SPairDataset returns only valid correspondences, but number of keypoints varies per pair.\n",
    "    max_kps = max(item['src_kps'].shape[0] for item in batch)\n",
    "\n",
    "    src_kps_padded = torch.full((len(batch), max_kps, 2), -1.0, dtype=torch.float32)\n",
    "    tgt_kps_padded = torch.full((len(batch), max_kps, 2), -1.0, dtype=torch.float32)\n",
    "    valid_mask_padded = torch.zeros((len(batch), max_kps), dtype=torch.bool)\n",
    "\n",
    "    for bi, item in enumerate(batch):\n",
    "        n = item['src_kps'].shape[0]\n",
    "        src_kps_padded[bi, :n] = item['src_kps']\n",
    "        tgt_kps_padded[bi, :n] = item['tgt_kps']\n",
    "        # Prefer the dataset-provided valid_mask if present; otherwise assume all n are valid\n",
    "        if 'valid_mask' in item and torch.is_tensor(item['valid_mask']):\n",
    "            vm = item['valid_mask']\n",
    "            valid_mask_padded[bi, :n] = vm[:n]\n",
    "        else:\n",
    "            valid_mask_padded[bi, :n] = True\n",
    "\n",
    "    out['src_kps'] = src_kps_padded\n",
    "    out['tgt_kps'] = tgt_kps_padded\n",
    "    out['valid_mask'] = valid_mask_padded\n",
    "\n",
    "    # 3) Collate the remaining fields\n",
    "    # Fixed-shape tensors -> stack\n",
    "    # strings / ids -> keep list\n",
    "    # variable shape tensors (rare) -> keep list\n",
    "    keys = list(batch[0].keys())\n",
    "    for k in keys:\n",
    "        if k in out:\n",
    "            continue\n",
    "        vals = [item[k] for item in batch]\n",
    "        v0 = vals[0]\n",
    "        if torch.is_tensor(v0):\n",
    "            try:\n",
    "                out[k] = torch.stack(vals, dim=0)\n",
    "            except RuntimeError:\n",
    "                out[k] = vals\n",
    "        else:\n",
    "            out[k] = vals\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83Y2Za4pNnIo"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 4: Fine-tuning Configuration\n",
    "# ============================================================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    \"\"\"Fine-tuning configuration.\"\"\"\n",
    "\n",
    "    # Model\n",
    "    backbone_name: str = 'dinov2_vitb14'  # 'dinov2_*', 'dinov3_*', 'sam_vit_b/l/h'\n",
    "    num_layers_to_finetune: int = 2      # last transformer blocks to unfreeze (ignored for SAM)\n",
    "\n",
    "    # Training\n",
    "    num_epochs: int = 10\n",
    "    learning_rate: float = 1e-5\n",
    "    head_lr_mult: float = 10.0   # head LR = learning_rate * head_lr_mult\n",
    "    proj_out_dim: Optional[int] = None  # None = keep same dim as backbone\n",
    "    use_projection_head: bool = True\n",
    "    weight_decay: float = 1e-2\n",
    "    warmup_epochs: int = 1\n",
    "\n",
    "    # Scheduler\n",
    "    use_scheduler: Optional[bool] = None   # if None: enabled only for DINOv2\n",
    "    step_lr_step_size: int = 5             # used only if scheduler enabled\n",
    "    step_lr_gamma: float = 0.5             # used only if scheduler enabled\n",
    "\n",
    "    # Loss\n",
    "    loss_type: str = 'cosine'  # 'cosine', 'l2', or 'combined'\n",
    "    negative_margin: float = 0.2\n",
    "\n",
    "    # Optimization\n",
    "    batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    max_grad_norm: float = 1.0\n",
    "\n",
    "    # Validation / checkpointing\n",
    "    validate_every_epochs: int = 1\n",
    "    val_num_samples: Optional[int] = None   # set an int for faster validation\n",
    "    pck_thresholds: List[float] = None\n",
    "    best_pck_threshold: float = 0.10\n",
    "    resume_from: Optional[str] = None       # path to last.pt\n",
    "\n",
    "    # Logging\n",
    "    log_interval: int = 50\n",
    "\n",
    "    # Paths\n",
    "    checkpoint_dir: str = FINETUNED_DIR\n",
    "    experiment_name: str = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.experiment_name is None:\n",
    "            self.experiment_name = f\"{self.backbone_name}_ft{self.num_layers_to_finetune}\"\n",
    "        if self.pck_thresholds is None:\n",
    "            self.pck_thresholds = [0.05, 0.10, 0.15, 0.20]\n",
    "        if self.use_scheduler is None:\n",
    "            self.use_scheduler = ('dinov2' in self.backbone_name)\n",
    "\n",
    "\n",
    "# Create configs for different experiments\n",
    "configs = {\n",
    "    # DINOv2: finetune last N blocks + scheduler\n",
    "    'dinov2_2layers': FinetuneConfig(\n",
    "        backbone_name='dinov2_vitb14',\n",
    "        num_layers_to_finetune=2,\n",
    "        num_epochs=10,\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=1e-2,\n",
    "        use_scheduler=True,\n",
    "        step_lr_step_size=5,\n",
    "        step_lr_gamma=0.5,\n",
    "    ),\n",
    "    'dinov2_4layers': FinetuneConfig(\n",
    "        backbone_name='dinov2_vitb14',\n",
    "        num_layers_to_finetune=4,\n",
    "        num_epochs=10,\n",
    "        learning_rate=5e-6,\n",
    "        weight_decay=1e-2,\n",
    "        use_scheduler=True,\n",
    "        step_lr_step_size=5,\n",
    "        step_lr_gamma=0.5,\n",
    "    ),\n",
    "\n",
    "    # DINOv3: finetune last N blocks, NO scheduler (keep LR very low)\n",
    "    'dinov3_2layers': FinetuneConfig(\n",
    "        backbone_name='dinov3_vitb16',\n",
    "        num_layers_to_finetune=2,\n",
    "        num_epochs=10,\n",
    "        learning_rate=5e-6,\n",
    "        weight_decay=5e-5,\n",
    "        use_scheduler=False,\n",
    "    ),\n",
    "\n",
    "    # SAM: image encoder frozen (num_layers_to_finetune ignored)\n",
    "    'sam_frozen': FinetuneConfig(\n",
    "        backbone_name='sam_vit_b',\n",
    "        num_layers_to_finetune=0,\n",
    "        num_epochs=10,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        use_scheduler=False,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Select config for this run\n",
    "config = configs['dinov2_2layers']\n",
    "\n",
    "print(f\"   Fine-tuning Configuration:\")\n",
    "print(f\"   Backbone: {config.backbone_name}\")\n",
    "print(f\"   Layers to finetune: {config.num_layers_to_finetune} (SAM ignores this)\")\n",
    "print(f\"   Epochs: {config.num_epochs}\")\n",
    "print(f\"   Learning rate: {config.learning_rate}\")\n",
    "print(f\"   Weight decay: {config.weight_decay}\")\n",
    "print(f\"   Scheduler enabled: {config.use_scheduler}\")\n",
    "print(f\"   Batch size: {config.batch_size}\")\n",
    "print(f\"   Validate every: {config.validate_every_epochs} epoch(s)\")\n",
    "print(f\"   Config ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQXj_zxxNq4-"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 5: Loss Functions for Correspondence\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CorrespondenceLoss(nn.Module):\n",
    "    \"\"\"Loss function for semantic correspondence fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss_type: str = 'cosine',\n",
    "        negative_margin: float = 0.2,\n",
    "        temperature: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.negative_margin = negative_margin\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src_features: torch.Tensor,\n",
    "        tgt_features: torch.Tensor,\n",
    "        src_kps: torch.Tensor,\n",
    "        tgt_kps: torch.Tensor,\n",
    "        valid_mask: torch.Tensor,\n",
    "        patch_size: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute correspondence loss.\n",
    "\n",
    "        Args:\n",
    "            src_features: (B, H_s, W_s, D)\n",
    "            tgt_features: (B, H_t, W_t, D)\n",
    "            src_kps: (B, N, 2) source keypoints\n",
    "            tgt_kps: (B, N, 2) target keypoints\n",
    "            valid_mask: (B, N) valid keypoint mask\n",
    "            patch_size: int\n",
    "\n",
    "        Returns:\n",
    "            loss: scalar tensor\n",
    "        \"\"\"\n",
    "        B = src_features.shape[0]\n",
    "        total_loss = 0.0\n",
    "        n_valid = 0\n",
    "\n",
    "        for b in range(B):\n",
    "            src_feat = src_features[b]  # (H_s, W_s, D)\n",
    "            tgt_feat = tgt_features[b]  # (H_t, W_t, D)\n",
    "            src_kp = src_kps[b]  # (N, 2)\n",
    "            tgt_kp = tgt_kps[b]  # (N, 2)\n",
    "            valid = valid_mask[b]  # (N,)\n",
    "\n",
    "            if valid.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # Filter valid keypoints\n",
    "            src_kp_valid = src_kp[valid]\n",
    "            tgt_kp_valid = tgt_kp[valid]\n",
    "\n",
    "            # Convert to patch coordinates\n",
    "            src_kp_patch = (src_kp_valid / patch_size).long()\n",
    "            tgt_kp_patch = (tgt_kp_valid / patch_size).long()\n",
    "\n",
    "            H_s, W_s, D = src_feat.shape\n",
    "            H_t, W_t, _ = tgt_feat.shape\n",
    "\n",
    "            # Clamp coordinates\n",
    "            src_kp_patch[:, 0] = src_kp_patch[:, 0].clamp(0, W_s - 1)\n",
    "            src_kp_patch[:, 1] = src_kp_patch[:, 1].clamp(0, H_s - 1)\n",
    "            tgt_kp_patch[:, 0] = tgt_kp_patch[:, 0].clamp(0, W_t - 1)\n",
    "            tgt_kp_patch[:, 1] = tgt_kp_patch[:, 1].clamp(0, H_t - 1)\n",
    "\n",
    "            # Extract features at keypoint locations\n",
    "            N = src_kp_valid.shape[0]\n",
    "            for i in range(N):\n",
    "                src_x, src_y = src_kp_patch[i]\n",
    "                tgt_x, tgt_y = tgt_kp_patch[i]\n",
    "\n",
    "                src_vec = src_feat[src_y, src_x]  # (D,)\n",
    "                tgt_vec = tgt_feat[tgt_y, tgt_x]  # (D,)\n",
    "\n",
    "                if self.loss_type == 'cosine':\n",
    "                    # Maximize cosine similarity between corresponding points\n",
    "                    similarity = F.cosine_similarity(\n",
    "                        src_vec.unsqueeze(0), tgt_vec.unsqueeze(0), dim=1\n",
    "                    )\n",
    "                    loss = 1.0 - similarity\n",
    "\n",
    "                elif self.loss_type == 'l2':\n",
    "                    # Minimize L2 distance\n",
    "                    loss = F.mse_loss(src_vec, tgt_vec)\n",
    "\n",
    "                elif self.loss_type == 'contrastive':\n",
    "                    # Contrastive loss: pull positives, push negatives\n",
    "                    # Positive: corresponding point\n",
    "                    pos_sim = F.cosine_similarity(\n",
    "                        src_vec.unsqueeze(0), tgt_vec.unsqueeze(0), dim=1\n",
    "                    )\n",
    "\n",
    "                    # Negatives: sample random points from target\n",
    "                    neg_indices = torch.randint(0, H_t * W_t, (8,), device=src_vec.device)\n",
    "                    tgt_flat = tgt_feat.reshape(-1, D)\n",
    "                    neg_vecs = tgt_flat[neg_indices]  # (8, D)\n",
    "\n",
    "                    neg_sim = F.cosine_similarity(\n",
    "                        src_vec.unsqueeze(0).expand(8, -1),\n",
    "                        neg_vecs,\n",
    "                        dim=1\n",
    "                    )\n",
    "\n",
    "                    # InfoNCE-style loss\n",
    "                    pos_exp = torch.exp(pos_sim / self.temperature)\n",
    "                    neg_exp = torch.exp(neg_sim / self.temperature).sum()\n",
    "\n",
    "                    loss = -torch.log(pos_exp / (pos_exp + neg_exp))\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n",
    "\n",
    "                total_loss += loss\n",
    "                n_valid += 1\n",
    "\n",
    "        if n_valid == 0:\n",
    "            return torch.tensor(0.0, device=src_features.device, requires_grad=True)\n",
    "\n",
    "        return total_loss / n_valid\n",
    "\n",
    "\n",
    "# Test loss function\n",
    "print(\" Testing loss function...\")\n",
    "\n",
    "loss_fn = CorrespondenceLoss(loss_type='cosine')\n",
    "\n",
    "# Dummy data\n",
    "src_feat = torch.randn(2, 37, 37, 768)\n",
    "tgt_feat = torch.randn(2, 37, 37, 768)\n",
    "src_kps = torch.randint(0, 500, (2, 10, 2)).float()\n",
    "tgt_kps = torch.randint(0, 500, (2, 10, 2)).float()\n",
    "valid_mask = torch.ones(2, 10).bool()\n",
    "\n",
    "loss = loss_fn(src_feat, tgt_feat, src_kps, tgt_kps, valid_mask, patch_size=14)\n",
    "print(f\" Loss computed: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OtVU3wGNuqY"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 6: Trainable Backbone Wrapper\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.backbones import DINOv2Extractor, DINOv3Extractor, SAMImageEncoder\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    # Simple, trainable projection head for patch features.\n",
    "    # - If out_dim is None -> identity-like (keeps same dimensionality)\n",
    "    # - Otherwise projects to out_dim and L2-normalizes (good for cosine similarity)\n",
    "    def __init__(self, in_dim: int, out_dim: int | None = None):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim if out_dim is not None else in_dim\n",
    "        self.proj = nn.Linear(self.in_dim, self.out_dim)\n",
    "        self.norm = nn.LayerNorm(self.out_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, H, W, D)\n",
    "        y = self.proj(x)\n",
    "        y = self.norm(y)\n",
    "        return torch.nn.functional.normalize(y, dim=-1)\n",
    "\n",
    "class FinetunableBackbone(nn.Module):\n",
    "    \"\"\"Wrapper to make backbone partially trainable.\n",
    "\n",
    "    NOTE: We keep feature extraction from the LAST layer (as in the baseline).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str,\n",
    "        num_layers_to_finetune: int = 2,\n",
    "        proj_out_dim: int | None = None,\n",
    "        use_projection_head: bool = True,\n",
    "        device: str = 'cuda'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone_name = backbone_name\n",
    "        self.num_layers_to_finetune = num_layers_to_finetune\n",
    "        self.device = device\n",
    "        self.proj_out_dim = proj_out_dim\n",
    "        self.use_projection_head = use_projection_head\n",
    "\n",
    "        # Load backbone extractor\n",
    "        if 'dinov2' in backbone_name:\n",
    "            self.extractor = DINOv2Extractor(\n",
    "                variant=backbone_name,\n",
    "                device=device,\n",
    "                allow_hub_download=True\n",
    "            )\n",
    "        elif 'dinov3' in backbone_name:\n",
    "            self.extractor = DINOv3Extractor(\n",
    "                variant=backbone_name,\n",
    "                device=device\n",
    "            )\n",
    "        elif 'sam' in backbone_name:\n",
    "            # backbone_name expected: 'sam_vit_b' / 'sam_vit_l' / 'sam_vit_h'\n",
    "            sam_variant = backbone_name.replace('sam_', '')\n",
    "            self.extractor = SAMImageEncoder(\n",
    "                variant=sam_variant,\n",
    "                checkpoint_path=None,\n",
    "                device=device,\n",
    "                allow_hub_download=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "\n",
    "        self.stride = self.extractor.stride\n",
    "\n",
    "        # Projection head (initialized lazily on first forward, because feature dim depends on backbone variant)\n",
    "        self.proj_head = None\n",
    "\n",
    "        # Freeze everything first\n",
    "        for p in self.extractor.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Unfreeze last N blocks ONLY for DINO backbones\n",
    "        if ('dinov2' in backbone_name) or ('dinov3' in backbone_name):\n",
    "            self._unfreeze_last_layers(num_layers_to_finetune)\n",
    "        else:\n",
    "            print(\"  SAM selected: image encoder kept frozen (no backbone fine-tuning).\")\n",
    "\n",
    "        # Count trainable parameters\n",
    "        n_trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        n_total = sum(p.numel() for p in self.parameters())\n",
    "\n",
    "        print(f\"   Trainable parameters:\")\n",
    "        print(f\"   Total: {n_total:,}\")\n",
    "        print(f\"   Trainable: {n_trainable:,} ({(n_trainable/n_total*100 if n_total>0 else 0):.2f}%)\")\n",
    "\n",
    "    def _unfreeze_last_layers(self, num_layers: int):\n",
    "        \"\"\"Unfreeze last N transformer blocks.\"\"\"\n",
    "        model = self.extractor.model\n",
    "\n",
    "        # Access transformer blocks\n",
    "        if hasattr(model, 'blocks'):\n",
    "            blocks = model.blocks\n",
    "        elif hasattr(model, 'encoder') and hasattr(model.encoder, 'layers'):\n",
    "            blocks = model.encoder.layers\n",
    "        else:\n",
    "            raise AttributeError(\"Cannot find transformer blocks in model\")\n",
    "\n",
    "        total_blocks = len(blocks)\n",
    "        start_idx = max(0, total_blocks - num_layers)\n",
    "\n",
    "        print(f\"  Unfreezing blocks {start_idx} to {total_blocks-1} (total: {total_blocks})\")\n",
    "\n",
    "        for i in range(start_idx, total_blocks):\n",
    "            for p in blocks[i].parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    def extract_features(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract features (with gradients if training).\"\"\"\n",
    "        feat_map, stride = self.extractor.extract_feats(image)\n",
    "        # (B, C, H, W) -> (B, H, W, C)\n",
    "        features = feat_map.permute(0, 2, 3, 1)\n",
    "\n",
    "        if self.use_projection_head:\n",
    "            if self.proj_head is None:\n",
    "                in_dim = features.shape[-1]\n",
    "                self.proj_head = ProjectionHead(in_dim, self.proj_out_dim).to(features.device)\n",
    "            features = self.proj_head(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        return self.extract_features(image)\n",
    "\n",
    "\n",
    "print(\" Backbone wrapper ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwDlfuheNxsa"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 7: Training Loop (with PCK validation, checkpoints, TensorBoard)\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from dataset.spair import compute_pck\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Fine-tuning trainer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: FinetunableBackbone,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        config: FinetuneConfig,\n",
    "        device: str = 'cuda'\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "\n",
    "        # Loss function\n",
    "        self.criterion = CorrespondenceLoss(\n",
    "            loss_type=config.loss_type,\n",
    "            negative_margin=config.negative_margin\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimizer (AdamW)\n",
    "        # Use param groups so the projection head can learn faster than the backbone.\n",
    "        head_lr = self.config.learning_rate * getattr(self.config, 'head_lr_mult', 10.0)\n",
    "\n",
    "        head_params = []\n",
    "        backbone_params = []\n",
    "\n",
    "        # Projection head (if present)\n",
    "        if hasattr(self.model, 'proj_head') and (self.model.proj_head is not None):\n",
    "            head_params = [p for p in self.model.proj_head.parameters() if p.requires_grad]\n",
    "\n",
    "        # Backbone trainable params (unfrozen blocks)\n",
    "        # Note: extractor params include both backbone and (possibly) other modules, but requires_grad filters correctly.\n",
    "        backbone_params = [p for p in self.model.extractor.parameters() if p.requires_grad]\n",
    "\n",
    "        # If proj_head is lazily created, we still set up optimizer for backbone now and will add head params later on first forward.\n",
    "        param_groups = []\n",
    "        if backbone_params:\n",
    "            param_groups.append({'params': backbone_params, 'lr': self.config.learning_rate})\n",
    "        if head_params:\n",
    "            param_groups.append({'params': head_params, 'lr': head_lr})\n",
    "\n",
    "        self.optimizer = optim.AdamW(\n",
    "            param_groups if param_groups else [p for p in model.parameters() if p.requires_grad],\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "\n",
    "        # Scheduler\n",
    "        self.scheduler = None\n",
    "        if config.use_scheduler:\n",
    "            # Warmup (linear) + StepLR\n",
    "            warmup_steps = max(1, config.warmup_epochs * len(train_loader))\n",
    "            warmup_scheduler = LinearLR(\n",
    "                self.optimizer,\n",
    "                start_factor=0.1,\n",
    "                total_iters=warmup_steps\n",
    "            )\n",
    "            step_scheduler = StepLR(\n",
    "                self.optimizer,\n",
    "                step_size=config.step_lr_step_size * len(train_loader),\n",
    "                gamma=config.step_lr_gamma\n",
    "            )\n",
    "            # Apply warmup first, then StepLR\n",
    "            self.scheduler = SequentialLR(\n",
    "                self.optimizer,\n",
    "                schedulers=[warmup_scheduler, step_scheduler],\n",
    "                milestones=[warmup_steps]\n",
    "            )\n",
    "\n",
    "        # State\n",
    "        self.global_step = 0\n",
    "        self.best_pck = -1.0\n",
    "\n",
    "        # Directories\n",
    "        self.ckpt_dir = Path(config.checkpoint_dir) / config.experiment_name\n",
    "        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.tb_dir = self.ckpt_dir / 'tb'\n",
    "        self.tb_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # TensorBoard writer\n",
    "        self.writer = SummaryWriter(log_dir=str(self.tb_dir))\n",
    "\n",
    "        print(f\"  Checkpoints: {self.ckpt_dir}\")\n",
    "        print(f\"  TensorBoard: {self.tb_dir}\")\n",
    "\n",
    "        # Optionally resume\n",
    "        if config.resume_from is not None:\n",
    "            self._resume_if_possible(config.resume_from)\n",
    "\n",
    "    def _resume_if_possible(self, ckpt_path: str):\n",
    "        ckpt_path = Path(ckpt_path)\n",
    "        if not ckpt_path.exists():\n",
    "            print(f\"  Resume requested, but checkpoint not found: {ckpt_path}\")\n",
    "            return\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        self.model.load_state_dict(ckpt['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        if self.scheduler is not None and ckpt.get('scheduler_state_dict') is not None:\n",
    "            self.scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "        self.global_step = ckpt.get('global_step', 0)\n",
    "        self.best_pck = ckpt.get('best_pck', -1.0)\n",
    "        start_epoch = ckpt.get('epoch', -1) + 1\n",
    "        print(f\"  Resumed from {ckpt_path} at epoch {start_epoch}, step {self.global_step}, best_pck {self.best_pck:.4f}\")\n",
    "\n",
    "    def _maybe_add_head_params(self):\n",
    "        # If projection head is created lazily during the first forward, add its params to optimizer once.\n",
    "        if not hasattr(self.model, 'proj_head') or self.model.proj_head is None:\n",
    "            return\n",
    "        # Check if already present\n",
    "        for pg in self.optimizer.param_groups:\n",
    "            if any(p is list(self.model.proj_head.parameters())[0] for p in pg['params'][:1]):\n",
    "                return\n",
    "        head_lr = self.config.learning_rate * getattr(self.config, 'head_lr_mult', 10.0)\n",
    "        self.optimizer.add_param_group({'params': [p for p in self.model.proj_head.parameters() if p.requires_grad], 'lr': head_lr})\n",
    "\n",
    "    def _log_lrs(self):\n",
    "        for i, pg in enumerate(self.optimizer.param_groups):\n",
    "            self.writer.add_scalar(f'lr/group_{i}', pg['lr'], self.global_step)\n",
    "\n",
    "    def train_epoch(self, epoch: int):\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            src_img = batch['src_img'].to(self.device)\n",
    "            tgt_img = batch['tgt_img'].to(self.device)\n",
    "            src_kps = batch['src_kps'].to(self.device)\n",
    "            tgt_kps = batch['tgt_kps'].to(self.device)\n",
    "            valid_mask = batch['valid_mask'].to(self.device)\n",
    "\n",
    "            src_features = self.model(src_img)\n",
    "            self._maybe_add_head_params()\n",
    "            tgt_features = self.model(tgt_img)\n",
    "\n",
    "            loss = self.criterion(\n",
    "                src_features, tgt_features,\n",
    "                src_kps, tgt_kps, valid_mask,\n",
    "                patch_size=self.model.stride\n",
    "            )\n",
    "\n",
    "            loss = loss / self.config.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(),\n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Stats\n",
    "            epoch_loss += loss.item() * self.config.gradient_accumulation_steps\n",
    "            n_batches += 1\n",
    "            self.global_step += 1\n",
    "\n",
    "            if self.global_step % self.config.log_interval == 0:\n",
    "                avg_loss = epoch_loss / n_batches\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.2e}'})\n",
    "\n",
    "                # TensorBoard\n",
    "                self.writer.add_scalar('train/loss_step', avg_loss, self.global_step)\n",
    "                self._log_lrs()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / max(1, n_batches)\n",
    "        self.writer.add_scalar('train/loss_epoch', avg_epoch_loss, epoch)\n",
    "        return avg_epoch_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate_pck(self, epoch: int):\n",
    "        \"\"\"Compute PCK@T on validation split (cosine similarity matching).\"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        all_scores = {f'PCK@{t:.2f}': [] for t in self.config.pck_thresholds}\n",
    "\n",
    "        n_processed = 0\n",
    "        pbar = tqdm(self.val_loader, desc='Validation (PCK)', leave=False)\n",
    "\n",
    "        for batch in pbar:\n",
    "            if self.config.val_num_samples is not None and n_processed >= self.config.val_num_samples:\n",
    "                break\n",
    "\n",
    "            src_img = batch['src_img'].to(self.device)\n",
    "            tgt_img = batch['tgt_img'].to(self.device)\n",
    "\n",
    "            # (K,2)\n",
    "            src_kps = batch['src_kps'][0]\n",
    "            tgt_kps = batch['tgt_kps'][0]\n",
    "            valid_mask = batch['valid_mask'][0]\n",
    "\n",
    "            src_kps_valid = src_kps[valid_mask]\n",
    "            tgt_kps_valid = tgt_kps[valid_mask]\n",
    "            if len(src_kps_valid) == 0:\n",
    "                continue\n",
    "\n",
    "            # Predict target kps\n",
    "            tgt_kps_pred = self._predict_keypoints(src_img, tgt_img, src_kps_valid)\n",
    "\n",
    "            H, W = tgt_img.shape[2:]\n",
    "            pck = compute_pck(tgt_kps_pred, tgt_kps_valid, (H, W), thresholds=self.config.pck_thresholds)\n",
    "\n",
    "            for k, v in pck.items():\n",
    "                all_scores[k].append(float(v))\n",
    "\n",
    "            n_processed += 1\n",
    "            if len(all_scores[f'PCK@{self.config.best_pck_threshold:.2f}']) > 0:\n",
    "                avg = float(np.mean(all_scores[f'PCK@{self.config.best_pck_threshold:.2f}']))\n",
    "                pbar.set_postfix({f'PCK@{self.config.best_pck_threshold:.2f}': f'{avg:.4f}'})\n",
    "\n",
    "        results = {k: (float(np.mean(v)) if len(v) else 0.0) for k, v in all_scores.items()}\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        for k, v in results.items():\n",
    "            self.writer.add_scalar(f'val/{k}', v, epoch)\n",
    "\n",
    "        return results\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _predict_keypoints(self, src_img: torch.Tensor, tgt_img: torch.Tensor, src_kps: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict target keypoints by cosine similarity argmax over target patches.\"\"\"\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        src_feat = self.model(src_img)[0]  # (Hs, Ws, D)\n",
    "        tgt_feat = self.model(tgt_img)[0]  # (Ht, Wt, D)\n",
    "\n",
    "        H_s, W_s, D = src_feat.shape\n",
    "        H_t, W_t, _ = tgt_feat.shape\n",
    "        patch_size = self.model.stride\n",
    "\n",
    "        src_kps_patch = (src_kps / patch_size).long()\n",
    "        src_kps_patch[:, 0] = src_kps_patch[:, 0].clamp(0, W_s - 1)\n",
    "        src_kps_patch[:, 1] = src_kps_patch[:, 1].clamp(0, H_s - 1)\n",
    "\n",
    "        N = src_kps.shape[0]\n",
    "        tgt_kps_pred = torch.zeros(N, 2, device=src_kps.device)\n",
    "\n",
    "        for i in range(N):\n",
    "            x, y = src_kps_patch[i]\n",
    "            src_vec = src_feat[y, x]\n",
    "\n",
    "            sim = F.cosine_similarity(\n",
    "                src_vec.view(1, 1, 1, D),\n",
    "                tgt_feat.unsqueeze(0),\n",
    "                dim=-1\n",
    "            ).squeeze(0)  # (Ht, Wt)\n",
    "\n",
    "            max_idx = sim.flatten().argmax()\n",
    "            pred_y = max_idx // W_t\n",
    "            pred_x = max_idx % W_t\n",
    "\n",
    "            tgt_kps_pred[i, 0] = pred_x * patch_size + patch_size // 2\n",
    "            tgt_kps_pred[i, 1] = pred_y * patch_size + patch_size // 2\n",
    "\n",
    "        return tgt_kps_pred\n",
    "\n",
    "    def _save_checkpoint(self, path: Path, epoch: int, extra: dict | None = None):\n",
    "        payload = {\n",
    "            'epoch': epoch,\n",
    "            'global_step': self.global_step,\n",
    "            'best_pck': self.best_pck,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': (self.scheduler.state_dict() if self.scheduler is not None else None),\n",
    "            'config': vars(self.config),\n",
    "        }\n",
    "        if extra:\n",
    "            payload.update(extra)\n",
    "        torch.save(payload, path)\n",
    "\n",
    "    def train(self):\n",
    "        print(f\"   Starting training: {self.config.experiment_name}\")\n",
    "        print(f\"   Epochs: {self.config.num_epochs}\")\n",
    "        print(f\"   Batches per epoch: {len(self.train_loader)}\")\n",
    "\n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            print(f\"  Epoch {epoch+1} - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Always save LAST checkpoint every epoch (safe resume)\n",
    "            last_path = self.ckpt_dir / 'last.pt'\n",
    "            self._save_checkpoint(last_path, epoch)\n",
    "\n",
    "            # Validation every N epochs (PCK)\n",
    "            if (epoch + 1) % self.config.validate_every_epochs == 0:\n",
    "                val_pck = self.validate_pck(epoch)\n",
    "                key = f\"PCK@{self.config.best_pck_threshold:.2f}\"\n",
    "                current = float(val_pck.get(key, 0.0))\n",
    "                print(f\"  Validation {key}: {current:.4f}\")\n",
    "\n",
    "                # Save best\n",
    "                if current > self.best_pck:\n",
    "                    self.best_pck = current\n",
    "                    best_path = self.ckpt_dir / 'best.pt'\n",
    "                    self._save_checkpoint(best_path, epoch, extra={'val_pck': val_pck})\n",
    "                    print(f\"  New best! Saved: {best_path} (best_pck={self.best_pck:.4f})\")\n",
    "\n",
    "        self.writer.close()\n",
    "        print(\"  Training complete!\")\n",
    "\n",
    "\n",
    "print(\" Trainer ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kwc7dQ3nN1m7"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 8: Launch Fine-tuning\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = FinetunableBackbone(\n",
    "    backbone_name=config.backbone_name,\n",
    "    num_layers_to_finetune=config.num_layers_to_finetune,\n",
    "    proj_out_dim=getattr(config, \"proj_out_dim\", None),\n",
    "    use_projection_head=getattr(config, \"use_projection_head\", True),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"To monitor TensorBoard in Colab, run:\")\n",
    "print(f\"  %load_ext tensorboard %tensorboard --logdir {trainer.tb_dir}\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dD_pFejLN71J"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 9: Evaluate Fine-tuned Model\n",
    "# ============================================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class FinetunedEvaluator:\n",
    "    \"\"\"Evaluator for fine-tuned models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: FinetunableBackbone,\n",
    "        dataloader: DataLoader,\n",
    "        device: str = 'cuda',\n",
    "        thresholds: list = [0.05, 0.10, 0.15, 0.20]\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.thresholds = thresholds\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, num_samples: int = None):\n",
    "        \"\"\"Evaluate fine-tuned model.\"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        all_pck = defaultdict(list)\n",
    "        per_category = defaultdict(lambda: defaultdict(list))\n",
    "        inference_times = []\n",
    "\n",
    "        n_processed = 0\n",
    "        pbar = tqdm(self.dataloader, desc=\"Evaluating\")\n",
    "\n",
    "        for batch in pbar:\n",
    "            if num_samples and n_processed >= num_samples:\n",
    "                break\n",
    "\n",
    "            src_img = batch['src_img'].to(self.device)\n",
    "            tgt_img = batch['tgt_img'].to(self.device)\n",
    "            src_kps = batch['src_kps'][0]\n",
    "            tgt_kps = batch['tgt_kps'][0]\n",
    "            valid_mask = batch['valid_mask'][0]\n",
    "            category = batch['category'][0]\n",
    "\n",
    "            src_kps_valid = src_kps[valid_mask]\n",
    "            tgt_kps_valid = tgt_kps[valid_mask]\n",
    "\n",
    "            if len(src_kps_valid) == 0:\n",
    "                continue\n",
    "\n",
    "            # Predict\n",
    "            start = time.time()\n",
    "            tgt_kps_pred = self._predict_keypoints(\n",
    "                src_img, tgt_img, src_kps_valid\n",
    "            )\n",
    "            inference_times.append(time.time() - start)\n",
    "\n",
    "            # Compute metrics\n",
    "            from dataset.spair import compute_pck\n",
    "            H, W = tgt_img.shape[2:]\n",
    "            pck_scores = compute_pck(\n",
    "                tgt_kps_pred, tgt_kps_valid, (H, W), thresholds=self.thresholds\n",
    "            )\n",
    "\n",
    "            for metric, value in pck_scores.items():\n",
    "                all_pck[metric].append(value)\n",
    "                per_category[category][metric].append(value)\n",
    "\n",
    "            n_processed += 1\n",
    "\n",
    "            if len(all_pck['PCK@0.10']) > 0:\n",
    "                avg_pck = np.mean(all_pck['PCK@0.10'])\n",
    "                pbar.set_postfix({'PCK@0.10': f'{avg_pck:.4f}'})\n",
    "\n",
    "        # Aggregate results\n",
    "        results = {\n",
    "            'overall': {},\n",
    "            'per_category': {},\n",
    "            'num_pairs': n_processed,\n",
    "            'inference_time_ms': np.mean(inference_times) * 1000\n",
    "        }\n",
    "\n",
    "        for metric in [f'PCK@{t:.2f}' for t in self.thresholds]:\n",
    "            values = all_pck[metric]\n",
    "            results['overall'][metric] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values)\n",
    "            }\n",
    "\n",
    "        for cat, metrics in per_category.items():\n",
    "            results['per_category'][cat] = {}\n",
    "            for metric in [f'PCK@{t:.2f}' for t in self.thresholds]:\n",
    "                results['per_category'][cat][metric] = np.mean(metrics[metric])\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _predict_keypoints(\n",
    "        self,\n",
    "        src_img: torch.Tensor,\n",
    "        tgt_img: torch.Tensor,\n",
    "        src_kps: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Predict target keypoints.\"\"\"\n",
    "        src_feat = self.model(src_img)[0]\n",
    "        tgt_feat = self.model(tgt_img)[0]\n",
    "\n",
    "        H_s, W_s, D = src_feat.shape\n",
    "        H_t, W_t, _ = tgt_feat.shape\n",
    "\n",
    "        patch_size = self.model.stride\n",
    "\n",
    "        src_kps_patch = (src_kps / patch_size).long()\n",
    "        src_kps_patch[:, 0] = src_kps_patch[:, 0].clamp(0, W_s - 1)\n",
    "        src_kps_patch[:, 1] = src_kps_patch[:, 1].clamp(0, H_s - 1)\n",
    "\n",
    "        N = src_kps.shape[0]\n",
    "        tgt_kps_pred = torch.zeros(N, 2, device=src_kps.device)\n",
    "\n",
    "        for i in range(N):\n",
    "            x, y = src_kps_patch[i]\n",
    "            src_vec = src_feat[y, x]\n",
    "\n",
    "            similarity = F.cosine_similarity(\n",
    "                src_vec.view(1, 1, 1, D),\n",
    "                tgt_feat.unsqueeze(0),\n",
    "                dim=-1\n",
    "            ).squeeze(0)\n",
    "\n",
    "            max_idx = similarity.flatten().argmax()\n",
    "            pred_y = max_idx // W_t\n",
    "            pred_x = max_idx % W_t\n",
    "\n",
    "            tgt_kps_pred[i, 0] = pred_x * patch_size + patch_size // 2\n",
    "            tgt_kps_pred[i, 1] = pred_y * patch_size + patch_size // 2\n",
    "\n",
    "        return tgt_kps_pred\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "evaluator = FinetunedEvaluator(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "results = evaluator.evaluate()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINE-TUNED MODEL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for metric, vals in results['overall'].items():\n",
    "    print(f\"   {metric}: {vals['mean']:.4f}  {vals['std']:.4f}\")\n",
    "print(f\"\\n  Inference: {results['inference_time_ms']:.2f} ms/pair\")\n",
    "print(f\" Evaluated: {results['num_pairs']} pairs\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save results\n",
    "results_path = f\"{RESULTS_DIR}/{config.experiment_name}_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"\\n Results saved: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJcot-zJOBAo"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 10: Compare Baseline vs Fine-tuned\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_comparison(baseline_results, finetuned_results):\n",
    "    \"\"\"Plot baseline vs fine-tuned comparison.\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    thresholds = [0.05, 0.10, 0.15, 0.20]\n",
    "\n",
    "    # Extract PCK values\n",
    "    baseline_pck = [baseline_results['overall'][f'PCK@{t:.2f}']['mean']\n",
    "                    for t in thresholds]\n",
    "    finetuned_pck = [finetuned_results['overall'][f'PCK@{t:.2f}']['mean']\n",
    "                     for t in thresholds]\n",
    "\n",
    "    # Plot 1: PCK Curves\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(thresholds, baseline_pck, marker='o', linewidth=2,\n",
    "             label='Baseline (Frozen)', markersize=8, color='#E74C3C')\n",
    "    ax1.plot(thresholds, finetuned_pck, marker='s', linewidth=2,\n",
    "             label=f'Fine-tuned ({config.num_layers_to_finetune} layers)',\n",
    "             markersize=8, color='#27AE60')\n",
    "\n",
    "    ax1.set_xlabel('Threshold', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('PCK', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('PCK Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 1])\n",
    "\n",
    "    # Plot 2: Improvement Bar Chart\n",
    "    ax2 = axes[1]\n",
    "    improvements = [(ft - bl) * 100 for bl, ft in zip(baseline_pck, finetuned_pck)]\n",
    "    colors = ['#27AE60' if imp > 0 else '#E74C3C' for imp in improvements]\n",
    "\n",
    "    bars = ax2.bar(range(len(thresholds)), improvements, color=colors, alpha=0.8)\n",
    "    ax2.set_xticks(range(len(thresholds)))\n",
    "    ax2.set_xticklabels([f'PCK@{t:.2f}' for t in thresholds])\n",
    "    ax2.set_ylabel('Improvement (%)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Fine-tuning Improvement', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    for bar, val in zip(bars, improvements):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, height,\n",
    "                f'{val:+.2f}%', ha='center',\n",
    "                va='bottom' if height > 0 else 'top',\n",
    "                fontsize=10, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = f'{RESULTS_DIR}/{config.experiment_name}_comparison.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n Saved: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load baseline results\n",
    "baseline_path = f'{RESULTS_DIR}/dinov2_vitb14_results.json'\n",
    "with open(baseline_path, 'r') as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "# Plot comparison\n",
    "plot_comparison(baseline_results, results)\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = []\n",
    "for t in [0.05, 0.10, 0.15, 0.20]:\n",
    "    metric = f'PCK@{t:.2f}'\n",
    "    bl = baseline_results['overall'][metric]['mean']\n",
    "    ft = results['overall'][metric]['mean']\n",
    "    imp = (ft - bl) * 100\n",
    "\n",
    "    comparison_data.append({\n",
    "        'Metric': metric,\n",
    "        'Baseline': f'{bl:.4f}',\n",
    "        'Fine-tuned': f'{ft:.4f}',\n",
    "        'Improvement': f'{imp:+.2f}%'\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
