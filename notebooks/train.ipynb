{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7f31a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T22:36:27.979687Z",
     "iopub.status.busy": "2026-01-25T22:36:27.978644Z",
     "iopub.status.idle": "2026-01-25T22:36:31.220797Z",
     "shell.execute_reply": "2026-01-25T22:36:31.219118Z",
     "shell.execute_reply.started": "2026-01-25T22:36:27.979628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CUDA available: True\n",
      "\n",
      "  GPU: Tesla V100-SXM2-16GB\n",
      "   VRAM: 16.9 GB\n",
      "\n",
      " Setup complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELLA 0: Setup directories and imports\n",
    "# ============================================================================\n",
    "from __future__ import annotations\n",
    "import sys, os, json, math, time, torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Literal\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n",
    "\n",
    "PROJECT_ROOT = str(Path.home() / \"AML\")\n",
    "LOCAL_REPO_NAME = str(Path.home() / \"AML_SemanticCorrespondence\")\n",
    "DATA_DIR = f'{PROJECT_ROOT}/dataset' \n",
    "CHECKPOINT_DIR = f'{PROJECT_ROOT}/checkpoints'\n",
    "RESULTS_DIR = f'{PROJECT_ROOT}/results'\n",
    "LOG_DIR = f'{PROJECT_ROOT}/logs'\n",
    "SPAIR_ROOT = f'{DATA_DIR}/Spair-71k'\n",
    "\n",
    "# Aggiungi al path la directory locale del repository\n",
    "sys.path.insert(0, LOCAL_REPO_NAME)\n",
    "\n",
    "# Ensure these directories exist (they will be created inside MyDrive/AML)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Local imports\n",
    "from models import DINOv2Extractor, DINOv3Extractor, SAMImageEncoder, CorrespondenceMatcher, UnifiedEvaluator, FinetunableBackbone, UnifiedBackbone, CorrespondenceLoss\n",
    "from dataset import SPairDataset\n",
    "from utils import compute_pck\n",
    "\n",
    "# Performance settings \n",
    "try:\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "except Exception:\n",
    "    pass\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Verifica GPU\n",
    "print(f\" CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"\\n  GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"\\n Setup complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a959e4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T22:36:31.225032Z",
     "iopub.status.busy": "2026-01-25T22:36:31.223328Z",
     "iopub.status.idle": "2026-01-25T22:36:33.444591Z",
     "shell.execute_reply": "2026-01-25T22:36:33.443154Z",
     "shell.execute_reply.started": "2026-01-25T22:36:31.224995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Installing dependencies...\n",
      "\n",
      "\n",
      " Dependencies installed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELLA 1: Install Dependencies\n",
    "# ============================================================================\n",
    "\n",
    "print(\" Installing dependencies...\\n\")\n",
    "!pip install -q -r {LOCAL_REPO_NAME}/requirements.txt\n",
    "print(\"\\n Dependencies installed!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ad408",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-25T22:36:33.446599Z",
     "iopub.status.busy": "2026-01-25T22:36:33.446250Z",
     "iopub.status.idle": "2026-01-25T22:36:33.459166Z",
     "shell.execute_reply": "2026-01-25T22:36:33.458233Z",
     "shell.execute_reply.started": "2026-01-25T22:36:33.446566Z"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1769274248554,
     "user": {
      "displayName": "Francesco Alzate",
      "userId": "15701221883649199981"
     },
     "user_tz": -60
    },
    "id": "1a1ad408",
    "outputId": "cf8058e7-7d96-4c8f-bfbe-e85a28fcee3f"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLA 2: Fine-tuning Configuration\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # -------------------------\n",
    "    # Model\n",
    "    # -------------------------\n",
    "    backbone_name: str = 'sam_vit_b'   # 'dinov2_vitb14', 'dinov3_vitb16', 'sam_vit_b/l/h'\n",
    "    num_layers_to_finetune: int = 2       # last transformer blocks to unfreeze\n",
    "\n",
    "    # -------------------------\n",
    "    # Training\n",
    "    # -------------------------\n",
    "    num_epochs: int = 5\n",
    "    learning_rate: float = 5e-6\n",
    "    weight_decay: float = 1e-2\n",
    "    warmup_epochs: int = 1\n",
    "    grad_clip_norm: float = 1.0\n",
    "    tqdm_update_interval: int = 100        # aggiorna tqdm/postfix ogni N batch\n",
    "\n",
    "    # Memory optimizations\n",
    "    use_amp: bool = True\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    use_gradient_checkpointing: bool = False   # implemented with best-effort (see model wrapper)\n",
    "\n",
    "    # DataLoader\n",
    "    batch_size: int = 1\n",
    "    num_workers: int = 0                       # keep 0 if dataset is on Drive\n",
    "    max_train_pairs: Optional[int] = None     # set None for full train\n",
    "    max_val_pairs: int = 1000                   # keep validation fast\n",
    "\n",
    "    # Loss\n",
    "    loss_type: str = 'contrastive'             # 'cosine' | 'l2' | 'contrastive'\n",
    "    temperature: float = 0.07\n",
    "\n",
    "    # -------------------------\n",
    "    # Evaluation modes\n",
    "    # -------------------------\n",
    "    val_mode: Literal[\"slice\", \"full\", \"hybrid\"] = \"hybrid\"\n",
    "    val_slice_max_batches: int = 50            # used in slice/hybrid\n",
    "    val_full_max_batches: Optional[int] = None # None = full validation\n",
    "    val_full_every: int = 1                    # run full val every N epochs (hybrid/full)\n",
    "\n",
    "    # -------------------------\n",
    "    # Logging / checkpoints\n",
    "    # -------------------------\n",
    "    use_tensorboard: bool = True\n",
    "    tb_logdir: str = \"runs/task2_finetune\"\n",
    "\n",
    "    use_wandb: bool = True\n",
    "    wandb_entity: str = \"luffy1\"\n",
    "    wandb_project: str = \"AML-project-semantic-correspondence\"\n",
    "\n",
    "    log_checkpoints_per_epoch: int = 10         # intra-epoch logs \n",
    "    run_name: str = \"\"                         # auto if empty\n",
    "\n",
    "    # Training mode\n",
    "    training_mode: Literal[\"fresh\", \"resume\", \"continue\"] = \"fresh\"\n",
    "    resume_checkpoint: str = \"\"                # used if training_mode == 'resume'\n",
    "\n",
    "config = FinetuneConfig()\n",
    "\n",
    "if not config.run_name:\n",
    "    config.run_name = f\"{config.backbone_name}_L{config.num_layers_to_finetune}_ls512_lr{config.learning_rate}\"\n",
    "\n",
    "# Align image size with main.ipynb\n",
    "def get_img_size(backbone_name: str) -> int:\n",
    "    if backbone_name.startswith(\"dinov2\"):\n",
    "        return 518\n",
    "    return 512\n",
    "\n",
    "IMG_SIZE = get_img_size(config.backbone_name if not config.backbone_name.startswith(\"sam\") else \"sam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891569c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "execution": {
     "iopub.execute_input": "2026-01-25T22:36:49.508259Z",
     "iopub.status.busy": "2026-01-25T22:36:49.507480Z",
     "iopub.status.idle": "2026-01-25T22:36:51.079031Z",
     "shell.execute_reply": "2026-01-25T22:36:51.077832Z",
     "shell.execute_reply.started": "2026-01-25T22:36:49.508217Z"
    },
    "executionInfo": {
     "elapsed": 3212,
     "status": "ok",
     "timestamp": 1769274251770,
     "user": {
      "displayName": "Francesco Alzate",
      "userId": "15701221883649199981"
     },
     "user_tz": -60
    },
    "id": "891569c8",
    "outputId": "3b702d7d-7581-483d-897a-1d7dc3869282"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/jupyter/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamuelecarrea\u001b[0m (\u001b[33msamuelecarrea-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/AML/notebooks/wandb/run-20260125_223649-l9wovgjy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luffy1/AML-project-semantic-correspondence/runs/l9wovgjy' target=\"_blank\">playful-gorge-52</a></strong> to <a href='https://wandb.ai/luffy1/AML-project-semantic-correspondence' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/luffy1/AML-project-semantic-correspondence' target=\"_blank\">https://wandb.ai/luffy1/AML-project-semantic-correspondence</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/luffy1/AML-project-semantic-correspondence/runs/l9wovgjy' target=\"_blank\">https://wandb.ai/luffy1/AML-project-semantic-correspondence/runs/l9wovgjy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb run started.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELLA 3: wandb init \n",
    "# ============================================================================\n",
    "import wandb\n",
    "if config.use_wandb:\n",
    "    # Verifica se sei gi√† loggato, altrimenti prova il login\n",
    "    try:\n",
    "        # Prova a ottenere l'API key esistente\n",
    "        wandb.login(relogin=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  WandB login necessario. Esegui 'wandb login' nel terminale se necessario.\")\n",
    "        # Modalit√† anonymous se il login fallisce (opzionale)\n",
    "        # wandb.init(anonymous=\"allow\")\n",
    "    \n",
    "    wandb.init(\n",
    "        entity=config.wandb_entity,\n",
    "        project=config.wandb_project,\n",
    "        config={\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"epochs\": config.num_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"num_layers_to_unfreeze\": config.num_layers_to_finetune,\n",
    "            \"model_to_finetune\": (\"sam\" if config.backbone_name.startswith(\"sam\") else (\"dinov2\" if config.backbone_name.startswith(\"dinov2\") else \"dinov3\")),\n",
    "            \"temperature\": config.temperature,\n",
    "            \"loss_type\": config.loss_type,\n",
    "            \"use_amp\": config.use_amp,\n",
    "            \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
    "            \"use_gradient_checkpointing\": config.use_gradient_checkpointing,\n",
    "            \"val_mode\": config.val_mode,\n",
    "        }\n",
    "    )\n",
    "    print(\"wandb run started.\")\n",
    "else:\n",
    "    print(\"wandb disabled by config.use_wandb=False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e6bf48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-25T22:36:58.446438Z",
     "iopub.status.busy": "2026-01-25T22:36:58.446033Z",
     "iopub.status.idle": "2026-01-25T22:36:58.562719Z",
     "shell.execute_reply": "2026-01-25T22:36:58.561733Z",
     "shell.execute_reply.started": "2026-01-25T22:36:58.446374Z"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1769274252402,
     "user": {
      "displayName": "Francesco Alzate",
      "userId": "15701221883649199981"
     },
     "user_tz": -60
    },
    "id": "b4e6bf48",
    "outputId": "aaaf7442-229f-4ee1-de32-f3fff85555cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 53340 pairs from train split (large)\n",
      " Loaded 5384 pairs from val split (large)\n",
      " Train pairs: 40000 | batches: 40000\n",
      "   Val pairs: 1000 | batches: 1000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELLA 7: DataLoaders (train/val)\n",
    "# ============================================================================\n",
    "\n",
    "def maybe_subset(ds, max_pairs):\n",
    "    if max_pairs is None:\n",
    "        return ds\n",
    "    idx = torch.randperm(len(ds))[:max_pairs].tolist()\n",
    "    return torch.utils.data.Subset(ds, idx)\n",
    "\n",
    "train_dataset_full = SPairDataset(\n",
    "    root=SPAIR_ROOT, split='train', size='large', long_side=IMG_SIZE,\n",
    "    normalize=True, load_segmentation=False\n",
    ")\n",
    "val_dataset_full = SPairDataset(\n",
    "    root=SPAIR_ROOT, split='val', size='large', long_side=IMG_SIZE,\n",
    "    normalize=True, load_segmentation=False\n",
    ")\n",
    "\n",
    "train_ds = maybe_subset(train_dataset_full, config.max_train_pairs)\n",
    "val_ds   = maybe_subset(val_dataset_full, config.max_val_pairs)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers,\n",
    "    pin_memory=torch.cuda.is_available(), drop_last=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers,\n",
    "    pin_memory=torch.cuda.is_available(), drop_last=False\n",
    ")\n",
    "print(f\" Train pairs: {len(train_ds)} | batches: {len(train_loader)}\")\n",
    "print(f\"   Val pairs: {len(val_ds)} | batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6af126",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-25T22:36:58.564553Z",
     "iopub.status.busy": "2026-01-25T22:36:58.563822Z",
     "iopub.status.idle": "2026-01-25T22:36:58.573630Z",
     "shell.execute_reply": "2026-01-25T22:36:58.572711Z",
     "shell.execute_reply.started": "2026-01-25T22:36:58.564520Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1769274252409,
     "user": {
      "displayName": "Francesco Alzate",
      "userId": "15701221883649199981"
     },
     "user_tz": -60
    },
    "id": "fc6af126",
    "outputId": "3df893e8-80c8-483c-b254-72b46a97fb28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval helpers ready (using UnifiedEvaluator + CorrespondenceMatcher).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELLA 8: Evaluation helpers ‚Äî Using UnifiedEvaluator + CorrespondenceMatcher\n",
    "# ============================================================================\n",
    "\n",
    "class FinetunableBackboneAdapter:\n",
    "    \"\"\"Adapter per usare FinetunableBackbone con CorrespondenceMatcher.\"\"\"\n",
    "    def __init__(self, finetunable_backbone):\n",
    "        self.model = finetunable_backbone\n",
    "        self.device = finetunable_backbone.device\n",
    "        self.config = type('obj', (object,), {'patch_size': finetunable_backbone.stride})()\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "        self.training = False\n",
    "        return self\n",
    "    \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.training = True\n",
    "        return self\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, img):\n",
    "        \"\"\"Adatta (B, H, W, D) ‚Üí (H, W, D) per compatibilit√† con CorrespondenceMatcher.\"\"\"\n",
    "        feat = self.model(img)  # (B, H, W, D)\n",
    "        return feat.squeeze(0),  # (H, W, D), restituisce tupla come expected\n",
    "\n",
    "evaluator = UnifiedEvaluator(\n",
    "    dataloader=val_loader,\n",
    "    device=device,\n",
    "    thresholds=[0.05, 0.10, 0.15, 0.20]\n",
    ")\n",
    "\n",
    "print(\" Eval helpers ready (using UnifiedEvaluator + CorrespondenceMatcher).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278fc5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T22:36:58.575335Z",
     "iopub.status.busy": "2026-01-25T22:36:58.575041Z",
     "iopub.status.idle": "2026-01-25T22:42:33.984341Z",
     "shell.execute_reply": "2026-01-25T22:42:33.982335Z",
     "shell.execute_reply.started": "2026-01-25T22:36:58.575306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Unfreezing last 2 blocks: [10..11] out of 12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_153529/1986034096.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model summary:\n",
      "  Backbone: sam_vit_b\n",
      "  Stride:   16\n",
      "  Feat dim: 256\n",
      "  Trainable params: 14,195,456 / 93,735,472 (15.14%)\n",
      "\n",
      " AMP: True | Grad accumulation: 8 | Grad checkpointing: False\n",
      " TensorBoard logdir: runs/task2_finetune/sam_vit_b_L2_ls512_lr5e-06\n",
      "\n",
      "üîß Training mode: fresh (starting from pretrained weights loaded by backbone extractor)\n",
      "\n",
      " Starting fine-tuning...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/5:   0%|          | 0/40000 [00:00<?, ?it/s]/var/tmp/ipykernel_153529/1986034096.py:291: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "Train 1/5:   4%|‚ñç         | 1700/40000 [05:31<2:04:01,  5.15it/s, loss=1.7503, lr=6.91e-07]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 373\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, config\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m    372\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 373\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     dt \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# Validation per config.val_mode\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 292\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    289\u001b[0m vm \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39muse_amp):\n\u001b[0;32m--> 292\u001b[0m     src_feat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     tgt_feat \u001b[38;5;241m=\u001b[39m model(tgt)\n\u001b[1;32m    294\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(src_feat, tgt_feat, src_kps, tgt_kps, vm, patch_size\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 143\u001b[0m, in \u001b[0;36mFinetunableBackbone.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 143\u001b[0m     feat_map, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feat_map\n",
      "File \u001b[0;32m~/AML_SemanticCorrespondence/models/backbones.py:324\u001b[0m, in \u001b[0;36mSAMImageEncoder.extract_feats\u001b[0;34m(self, image, return_padding)\u001b[0m\n\u001b[1;32m    316\u001b[0m resized_img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m    317\u001b[0m     image,\n\u001b[1;32m    318\u001b[0m     size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size),\n\u001b[1;32m    319\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    320\u001b[0m     align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    321\u001b[0m )\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Extract features at fixed resolution\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized_img\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, D, H_sam, W_sam)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# Resize features back to match original aspect ratio\u001b[39;00m\n\u001b[1;32m    327\u001b[0m _, _, feat_H, feat_W \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/AML_SemanticCorrespondence/models/backbones.py:347\u001b[0m, in \u001b[0;36mSAMImageEncoder._forward_features\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through SAM encoder. Returns (B, D, H, W).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:349\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    347\u001b[0m q_h, q_w \u001b[38;5;241m=\u001b[39m q_size\n\u001b[1;32m    348\u001b[0m k_h, k_w \u001b[38;5;241m=\u001b[39m k_size\n\u001b[0;32m--> 349\u001b[0m Rh \u001b[38;5;241m=\u001b[39m \u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m Rw \u001b[38;5;241m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    352\u001b[0m B, _, dim \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELLA 9: Training Loop (AMP + Warmup + Grad Clip + TB + Resume + Val modes)\n",
    "# ============================================================================\n",
    "\n",
    "# -------------------------\n",
    "# Build finetunable model\n",
    "# -------------------------\n",
    "model = FinetunableBackbone(\n",
    "    backbone_name=config.backbone_name,\n",
    "    num_layers_to_finetune=config.num_layers_to_finetune,\n",
    "    device=device,\n",
    "    use_gradient_checkpointing=config.use_gradient_checkpointing,\n",
    ").to(device)\n",
    "\n",
    "# Loss (repo)\n",
    "criterion = CorrespondenceLoss(loss_type=config.loss_type, temperature=config.temperature).to(device)\n",
    "print(\" Loss:\", config.loss_type)\n",
    "\n",
    "# -------------------------\n",
    "# Optimizer\n",
    "# -------------------------\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "if len(trainable_params) == 0:\n",
    "    raise RuntimeError(\"No trainable parameters found. Check num_layers_to_finetune > 0.\")\n",
    "\n",
    "optimizer = optim.AdamW(trainable_params, lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "# -------------------------\n",
    "# Scheduler: warmup -> cosine (step-based)\n",
    "# -------------------------\n",
    "total_updates = max(1, (len(train_loader) * config.num_epochs) // config.gradient_accumulation_steps)\n",
    "warmup_updates = max(1, (len(train_loader) * config.warmup_epochs) // config.gradient_accumulation_steps)\n",
    "\n",
    "warmup = LinearLR(optimizer, start_factor=0.1, total_iters=warmup_updates)\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=max(1, total_updates - warmup_updates))\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_updates])\n",
    "\n",
    "# AMP\n",
    "use_amp = bool(config.use_amp and torch.cuda.is_available())\n",
    "scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "print(f\" AMP: {use_amp} | Grad accumulation: {config.gradient_accumulation_steps} | Grad checkpointing: {config.use_gradient_checkpointing}\")\n",
    "\n",
    "# TensorBoard\n",
    "tb_writer = None\n",
    "if config.use_tensorboard:\n",
    "    logdir = Path(config.tb_logdir) / config.run_name\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    tb_writer = SummaryWriter(log_dir=str(logdir))\n",
    "    print(f\" TensorBoard logdir: {logdir}\")\n",
    "\n",
    "# -------------------------\n",
    "# File Logging setup\n",
    "# -------------------------\n",
    "log_dir_path = Path(LOG_DIR)\n",
    "log_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "log_filename = log_dir_path / f\"train_{config.run_name}_{time.strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logger = logging.getLogger(f\"finetune_{config.run_name}\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    fh = logging.FileHandler(str(log_filename))\n",
    "    fh.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    logger.propagate = False\n",
    "logger.info(f\"Starting training: run_name={config.run_name} backbone={config.backbone_name} epochs={config.num_epochs} lr={config.learning_rate}\")\n",
    "\n",
    "# -------------------------\n",
    "# Checkpointing (complete state for resume)\n",
    "# -------------------------\n",
    "best_pck = -1.0\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "best_path = Path(CHECKPOINT_DIR) / f\"finetuned_{config.run_name}.pt\"\n",
    "best_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_ckpt(path: Path, epoch: int, best_pck_val: float):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"scaler_state_dict\": scaler.state_dict() if use_amp else None,\n",
    "        \"best_pck@0.10\": best_pck_val,\n",
    "        \"config\": config.__dict__,\n",
    "    }, str(path))\n",
    "\n",
    "if config.training_mode == \"resume\":\n",
    "    ckpt_path = config.resume_checkpoint or str(best_path)\n",
    "    if ckpt_path and os.path.exists(ckpt_path):\n",
    "        print(f\"\\nüîß Resuming from checkpoint: {ckpt_path}\")\n",
    "        logger.info(f\"Resuming from checkpoint: {ckpt_path}\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "        if use_amp and ckpt.get(\"scaler_state_dict\") is not None:\n",
    "            scaler.load_state_dict(ckpt[\"scaler_state_dict\"])\n",
    "        start_epoch = int(ckpt.get(\"epoch\", 0)) + 1\n",
    "        global_step = int(ckpt.get(\"global_step\", 0))\n",
    "        best_pck = float(ckpt.get(\"best_pck@0.10\", best_pck))\n",
    "        print(f\"   ‚úì start_epoch={start_epoch} | global_step={global_step} | best_pck@0.10={best_pck:.2f}\")\n",
    "        logger.info(f\"Resume stats: start_epoch={start_epoch} global_step={global_step} best_pck@0.10={best_pck:.2f}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è resume requested but checkpoint not found: {ckpt_path}\")\n",
    "        print(\"   Continuing from current weights in memory.\")\n",
    "        logger.warning(f\"Resume requested but checkpoint not found: {ckpt_path}. Continuing.\")\n",
    "elif config.training_mode == \"fresh\":\n",
    "    print(\"\\nüîß Training mode: fresh (starting from pretrained weights loaded by backbone extractor)\")\n",
    "    logger.info(\"Training mode: fresh\")\n",
    "else:\n",
    "    print(\"\\nüîß Training mode: continue (keeping current weights in memory)\")\n",
    "    logger.info(\"Training mode: continue\")\n",
    "\n",
    "# -------------------------\n",
    "# Intra-epoch log checkpoints\n",
    "# -------------------------\n",
    "def get_log_steps(n_batches: int, k: int):\n",
    "    if k <= 0:\n",
    "        return set()\n",
    "    if n_batches <= 1:\n",
    "        return {0}\n",
    "    steps = set(int(round(x)) for x in np.linspace(0, n_batches - 1, k))\n",
    "    return steps\n",
    "\n",
    "log_steps = get_log_steps(len(train_loader), config.log_checkpoints_per_epoch)\n",
    "\n",
    "# -------------------------\n",
    "# Validation function using UnifiedEvaluator\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def run_validation_unified(model_wrapper, num_samples=None):\n",
    "    \"\"\"Run validation using UnifiedEvaluator (consistent with eval.ipynb)\"\"\"\n",
    "    matcher = CorrespondenceMatcher(model_wrapper, use_soft_argmax=False)\n",
    "    \n",
    "    results = evaluator.evaluate(\n",
    "        matcher=matcher,\n",
    "        backbone_name=f\"finetuned_{config.backbone_name}\",\n",
    "        num_samples=num_samples,\n",
    "        show_progress=False\n",
    "    )\n",
    "    \n",
    "    # Extract PCK@0.10 and loss\n",
    "    pck_010 = results['overall_keypoint']['PCK@0.10']['mean'] * 100.0\n",
    "    \n",
    "    return pck_010, results\n",
    "\n",
    "# -------------------------\n",
    "# One epoch\n",
    "# -------------------------\n",
    "def train_one_epoch(epoch: int):\n",
    "    global global_step\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    pbar = tqdm(total=len(train_loader), desc=f\"Train {epoch+1}/{config.num_epochs}\", leave=False)\n",
    "\n",
    "    accumulated = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        src = batch[\"src_img\"].to(device, non_blocking=True)\n",
    "        tgt = batch[\"tgt_img\"].to(device, non_blocking=True)\n",
    "        src_kps = batch[\"src_kps\"].to(device, non_blocking=True)\n",
    "        tgt_kps = batch[\"tgt_kps\"].to(device, non_blocking=True)\n",
    "        vm = batch[\"valid_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "        with autocast(enabled=use_amp):\n",
    "            src_feat = model(src)\n",
    "            tgt_feat = model(tgt)\n",
    "            loss = criterion(src_feat, tgt_feat, src_kps, tgt_kps, vm, patch_size=model.stride)\n",
    "            loss = loss / config.gradient_accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        accumulated += 1\n",
    "\n",
    "        do_update = (accumulated == config.gradient_accumulation_steps) or (step + 1 == len(train_loader))\n",
    "\n",
    "        if do_update:\n",
    "            if config.grad_clip_norm is not None and config.grad_clip_norm > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip_norm)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            scheduler.step()\n",
    "            accumulated = 0\n",
    "            global_step += 1\n",
    "\n",
    "        epoch_losses.append(float(loss.item() * config.gradient_accumulation_steps))\n",
    "\n",
    "        should_refresh = ((step + 1) % config.tqdm_update_interval == 0) or ((step + 1) == len(train_loader))\n",
    "        if should_refresh:\n",
    "            pbar.update((step + 1) - pbar.n)\n",
    "            try:\n",
    "                lr_now = scheduler.get_last_lr()[0]\n",
    "            except Exception:\n",
    "                lr_now = optimizer.param_groups[0].get('lr', config.learning_rate)\n",
    "            pbar.set_postfix({\"loss\": f\"{np.mean(epoch_losses):.4f}\", \"lr\": f\"{lr_now:.2e}\"})\n",
    "            logger.info(f\"epoch={epoch+1} step={step+1}/{len(train_loader)} loss={np.mean(epoch_losses):.4f} lr={lr_now:.2e} global_step={global_step}\")\n",
    "\n",
    "\n",
    "        if config.use_wandb and (step in log_steps) and (step > 0):\n",
    "            avg_train_loss = float(np.mean(epoch_losses)) if len(epoch_losses) else 0.0\n",
    "            \n",
    "            t_eval0 = time.time()\n",
    "            model_wrapper = FinetunableBackboneAdapter(model)\n",
    "            score, _ = run_validation_unified(model_wrapper, num_samples=10)  # solo 10 batch per speed\n",
    "            t_eval = time.time() - t_eval0\n",
    "\n",
    "            pbar.write(\n",
    "                f\"[Intra-epoch eval] epoch={epoch+1} step={step+1}/{len(train_loader)} | \"\n",
    "                f\"PCK@0.10={score:.2f} | {t_eval:.1f}s\"\n",
    "            )\n",
    "            \n",
    "            wandb.log({\n",
    "                \"training loss\": avg_train_loss,\n",
    "                \"validation pck\": score,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": step + 1,\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            model.train()  # ritorna in train mode\n",
    "\n",
    "    pbar.update(len(train_loader) - pbar.n)\n",
    "    pbar.close()\n",
    "    return float(np.mean(epoch_losses)) if len(epoch_losses) else 0.0\n",
    "\n",
    "print(\"\\n Starting fine-tuning...\\n\")\n",
    "logger.info(\"Starting fine-tuning loop\")\n",
    "\n",
    "for epoch in range(start_epoch, config.num_epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss = train_one_epoch(epoch)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    model_wrapper = FinetunableBackboneAdapter(model)\n",
    "    \n",
    "    if config.val_mode == \"slice\":\n",
    "        num_samples = config.val_slice_max_batches\n",
    "    elif config.val_mode == \"full\":\n",
    "        num_samples = config.val_full_max_batches if (epoch + 1) % config.val_full_every == 0 else None\n",
    "    else:  # hybrid\n",
    "        num_samples = config.val_slice_max_batches if (epoch + 1) % config.val_full_every != 0 else config.val_full_max_batches\n",
    "\n",
    "    score, val_results = run_validation_unified(model_wrapper, num_samples=num_samples)\n",
    "\n",
    "    print(f\"\\n Epoch {epoch+1}/{config.num_epochs} | train_loss={train_loss:.4f} | PCK@0.10={score:.2f} | {dt:.1f}s\")\n",
    "    logger.info(f\"Epoch summary: epoch={epoch+1} train_loss={train_loss:.4f} PCK@0.10={score:.2f} time={dt:.1f}s\")\n",
    "\n",
    "    # TensorBoard\n",
    "    if tb_writer is not None:\n",
    "        tb_writer.add_scalar(\"train/loss\", train_loss, epoch)\n",
    "        tb_writer.add_scalar(\"val/pck@0.10\", score, epoch)\n",
    "\n",
    "    # wandb end-of-epoch logging\n",
    "    if config.use_wandb:\n",
    "        wandb.log({\n",
    "            \"training loss\": train_loss,\n",
    "            \"validation pck\": score,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "        })\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if score > best_pck:\n",
    "        best_pck = score\n",
    "        save_ckpt(best_path, epoch, best_pck)\n",
    "        print(f\"  ‚úî Saved best checkpoint: {best_path} (PCK@0.10={best_pck:.2f})\")\n",
    "        logger.info(f\"Saved best checkpoint: path={best_path} PCK@0.10={best_pck:.2f}\")\n",
    "\n",
    "print(\"\\n Fine-tuning done.\")\n",
    "print(f\" Best PCK@0.10: {best_pck:.2f}\")\n",
    "logger.info(f\"Fine-tuning done. Best PCK@0.10={best_pck:.2f}\")\n",
    "\n",
    "if tb_writer is not None:\n",
    "    tb_writer.close()\n",
    "\n",
    "# finish wandb run\n",
    "if config.use_wandb:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26639085-73b6-49e9-906a-4c5d7ef9e489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m138",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m138"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
