{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQjXxvV3KYBJ"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELLA 1: Setup & Mount Google Drive\n",
        "# ============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\" Task 4: PF-WILLOW Zero-Shot Evaluation\")\n",
        "print(\" Using trained model from Task 2\\n\")\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "if not Path('/content/drive').exists():\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted\\n\")\n",
        "else:\n",
        "    print(\"‚úÖ Google Drive already mounted\\n\")\n",
        "\n",
        "# 2. Setup directories\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/AML'\n",
        "DATA_DIR = f'{PROJECT_ROOT}/dataset'\n",
        "CHECKPOINT_DIR = f'{PROJECT_ROOT}/checkpoints'\n",
        "RESULTS_DIR = f'{PROJECT_ROOT}/results'\n",
        "FIGURES_DIR = f'{PROJECT_ROOT}/results/figures'\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [DATA_DIR, CHECKPOINT_DIR, RESULTS_DIR, FIGURES_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(f\" Project root: {PROJECT_ROOT}\")\n",
        "print(f\" Checkpoints: {CHECKPOINT_DIR}\")\n",
        "print(f\" Results: {RESULTS_DIR}\\n\")\n",
        "\n",
        "# 3. Clone repository\n",
        "GITHUB_REPO_URL = 'https://github.com/SamueleCarrea/AML_SemanticCorrespondence'\n",
        "LOCAL_REPO_NAME = 'AML_SemanticCorrespondence'\n",
        "\n",
        "if not Path(LOCAL_REPO_NAME).exists():\n",
        "    print(f\" Cloning repository...\")\n",
        "    !git clone {GITHUB_REPO_URL} {LOCAL_REPO_NAME}\n",
        "    print(\"‚úÖ Repository cloned\")\n",
        "else:\n",
        "    print(f\"‚úÖ Repository already exists\")\n",
        "    if Path(LOCAL_REPO_NAME, '.git').exists():\n",
        "        print(\"üîÑ Pulling latest changes...\")\n",
        "        %cd {LOCAL_REPO_NAME}\n",
        "        !git pull\n",
        "        %cd ..\n",
        "\n",
        "sys.path.insert(0, LOCAL_REPO_NAME)\n",
        "\n",
        "# 4. Check GPU\n",
        "import torch\n",
        "print(f\"\\n  GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"\\n‚úÖ Setup complete!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 2: Install Dependencies\n",
        "# ============================================================================\n",
        "\n",
        "print(\" Installing dependencies...\\n\")\n",
        "\n",
        "# Install requirements\n",
        "!pip install -q -r {LOCAL_REPO_NAME}/requirements.txt\n",
        "!pip install -q scipy pandas pillow tqdm matplotlib seaborn\n",
        "\n",
        "# Additional packages if needed for Task 2 model\n",
        "!pip install -q pytorch-lightning\n",
        "\n",
        "# Verify\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "print(f\"\\n PyTorch {torch.__version__}\")\n",
        "print(f\" Lightning {pl.__version__}\")\n",
        "print(f\" CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "print(\"\\n Dependencies installed!\\n\")"
      ],
      "metadata": {
        "id": "4Ng3sTwTRFWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 3: Download PF-WILLOW Dataset\n",
        "# ============================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "WILLOW_ROOT = f'{DATA_DIR}/PF-WILLOW'\n",
        "\n",
        "print(f\"‚úÖ Dataset exists at {WILLOW_ROOT}\")\n",
        "\n",
        "# Verify structure\n",
        "print(\"\\n Dataset verification:\")\n",
        "willow_path = Path(WILLOW_ROOT)\n",
        "\n",
        "# Count files in each subset\n",
        "subsets = [\n",
        "    'car(S)', 'car(M)', 'car(G)',\n",
        "    'duck(S)',\n",
        "    'motorbike(S)', 'motorbike(M)', 'motorbike(G)',\n",
        "    'winebottle(wC)', 'winebottle(woC)', 'winebottle(M)'\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Subset':<25} {'Images':<10} {'Annotations':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for subset in subsets:\n",
        "    subset_dir = willow_path / subset\n",
        "    if subset_dir.exists():\n",
        "        n_png = len(list(subset_dir.glob('*.png')))\n",
        "        n_mat = len(list(subset_dir.glob('*.mat')))\n",
        "        print(f\"{subset:<25} {n_png:<10} {n_mat:<10}\")\n",
        "\n",
        "# Check test_pairs.csv\n",
        "if (willow_path / 'test_pairs.csv').exists():\n",
        "    import pandas as pd\n",
        "    pairs = pd.read_csv(willow_path / 'test_pairs.csv')\n",
        "    print(f\"\\n‚úÖ Found {len(pairs)} test pairs in test_pairs.csv\")\n",
        "    print(f\"‚úÖ Categories: {pairs['category'].unique().tolist()}\")\n",
        "else:\n",
        "    print(\"\\n  WARNING: test_pairs.csv not found!\")\n",
        "\n",
        "print(\"\\n‚úÖ Dataset ready!\\n\")"
      ],
      "metadata": {
        "id": "IAEEDnfBRFiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 4: Load PF-WILLOW Dataset\n",
        "# ============================================================================\n",
        "\n",
        "from dataset.willow import PFWillowDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "WILLOW_ROOT = f'{DATA_DIR}/PF-WILLOW'\n",
        "\n",
        "print(\" Loading PF-WILLOW dataset...\\n\")\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = PFWillowDataset(\n",
        "    root=WILLOW_ROOT,\n",
        "    long_side=518,\n",
        "    normalize=True\n",
        ")\n",
        "\n",
        "# Create DataLoader (batch_size=1 importante!)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=PFWillowDataset.collate_fn\n",
        ")\n",
        "\n",
        "# Sanity check\n",
        "sample = test_dataset[0]\n",
        "print(f\"\\n Dataset Info:\")\n",
        "print(f\"  Total pairs: {len(test_dataset)}\")\n",
        "print(f\"  Categories: {test_dataset.categories}\")\n",
        "print(f\"  Keypoints per pair: {test_dataset.N_KEYPOINTS}\")\n",
        "\n",
        "print(f\"\\n Sample pair:\")\n",
        "print(f\"  Category: {sample['category']}\")\n",
        "print(f\"  Source image shape: {sample['src_img'].shape}\")\n",
        "print(f\"  Target image shape: {sample['tgt_img'].shape}\")\n",
        "print(f\"  Source keypoints: {sample['src_kps'].shape}\")\n",
        "print(f\"  Valid keypoints: {sample['valid_mask'].sum().item()}/{len(sample['valid_mask'])}\")\n",
        "\n",
        "print(\"\\n‚úÖ Dataset loaded successfully!\")"
      ],
      "metadata": {
        "id": "q4Q4aKlvRFk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 5: Load Your Trained Model from Task 2\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# IMPORTANTE: Aggiorna questi path con i tuoi file\n",
        "CHECKPOINT_PATH = f\"{CHECKPOINT_DIR}/best_model_task2.ckpt\"\n",
        "\n",
        "# Verifica che il checkpoint esista\n",
        "if not Path(CHECKPOINT_PATH).exists():\n",
        "    print(f\" Checkpoint not found: {CHECKPOINT_PATH}\")\n",
        "    print(f\"\\n Available checkpoints in {CHECKPOINT_DIR}:\")\n",
        "    for ckpt in Path(CHECKPOINT_DIR).glob(\"*.ckpt\"):\n",
        "        print(f\"  - {ckpt.name}\")\n",
        "\n",
        "    # Se hai un checkpoint con nome diverso, aggiornalo qui:\n",
        "    print(f\"\\n Update CHECKPOINT_PATH variable with your actual checkpoint name\")\n",
        "else:\n",
        "    print(f\"‚úÖ Checkpoint found: {CHECKPOINT_PATH}\")\n",
        "\n",
        "# IMPORTANTE: Importa il tuo modello Task 2\n",
        "# Sostituisci questa linea con il tuo import reale:\n",
        "\n",
        "# Opzione 1: Se il tuo modello √® in models/correspondence_model.py\n",
        "# from models.correspondence_model import YourModelClass\n",
        "\n",
        "# Opzione 2: Se usi un file diverso\n",
        "# sys.path.insert(0, f'{LOCAL_REPO_NAME}/models')\n",
        "# from your_model_file import YourModelClass\n",
        "\n",
        "# Per ora uso un placeholder - DEVI CAMBIARE QUESTO!\n",
        "print(\"\\n‚ö†Ô∏è  ATTENZIONE: Devi aggiornare l'import del modello!\")\n",
        "print(\"   Vedi commenti nella cella per istruzioni\\n\")\n",
        "\n",
        "# ESEMPIO DI CARICAMENTO (aggiorna con il tuo modello reale):\n",
        "# model = YourModelClass.load_from_checkpoint(CHECKPOINT_PATH)\n",
        "\n",
        "# Placeholder per evitare errori (RIMUOVI QUESTO):\n",
        "class PlaceholderModel(pl.LightningModule):\n",
        "    def predict(self, src_img, tgt_img, src_kps):\n",
        "        # Questo √® solo un placeholder!\n",
        "        # Ritorna keypoints casuali - NON USARE IN PRODUZIONE\n",
        "        return src_kps + torch.randn_like(src_kps) * 10\n",
        "\n",
        "# SOSTITUISCI con:\n",
        "# model = YourModelClass.load_from_checkpoint(CHECKPOINT_PATH)\n",
        "model = PlaceholderModel()\n",
        "\n",
        "print(\"‚úÖ Model loaded (update with your real model!)\\n\")"
      ],
      "metadata": {
        "id": "CSJE7OOxRFni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 6: PCK Metric Implementation\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "def compute_pck(\n",
        "    pred_kps: torch.Tensor,\n",
        "    gt_kps: torch.Tensor,\n",
        "    image_size: tuple,\n",
        "    thresholds: List[float] = [0.05, 0.10, 0.15, 0.20]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute Percentage of Correct Keypoints (PCK).\n",
        "\n",
        "    Args:\n",
        "        pred_kps: (N, 2) predicted keypoints (x, y)\n",
        "        gt_kps: (N, 2) ground truth keypoints (x, y)\n",
        "        image_size: (H, W) image dimensions\n",
        "        thresholds: List of alpha values\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with PCK@alpha for each threshold\n",
        "    \"\"\"\n",
        "    H, W = image_size\n",
        "    max_dim = max(H, W)\n",
        "\n",
        "    # Compute Euclidean distance\n",
        "    distances = torch.norm(pred_kps - gt_kps, dim=1)  # (N,)\n",
        "\n",
        "    # Normalize by max image dimension\n",
        "    normalized_distances = distances / max_dim\n",
        "\n",
        "    # Compute PCK for each threshold\n",
        "    results = {}\n",
        "    for alpha in thresholds:\n",
        "        correct = (normalized_distances <= alpha).float()\n",
        "        pck = correct.mean().item() * 100  # Convert to percentage\n",
        "        results[f'PCK@{alpha:.2f}'] = pck\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Test metrics\n",
        "print(\" Testing PCK computation...\\n\")\n",
        "\n",
        "# Example: perfect prediction\n",
        "pred_perfect = torch.tensor([[100.0, 150.0], [200.0, 250.0]])\n",
        "gt_perfect = torch.tensor([[100.0, 150.0], [200.0, 250.0]])\n",
        "img_size = (480, 640)\n",
        "\n",
        "pck_perfect = compute_pck(pred_perfect, gt_perfect, img_size)\n",
        "print(\"Perfect prediction:\")\n",
        "for metric, value in pck_perfect.items():\n",
        "    print(f\"  {metric}: {value:.2f}%\")\n",
        "\n",
        "# Example: imperfect prediction\n",
        "pred_imperfect = torch.tensor([[105.0, 155.0], [195.0, 245.0]])\n",
        "pck_imperfect = compute_pck(pred_imperfect, gt_perfect, img_size)\n",
        "print(\"\\nImperfect prediction (5px error):\")\n",
        "for metric, value in pck_imperfect.items():\n",
        "    print(f\"  {metric}: {value:.2f}%\")\n",
        "\n",
        "print(\"\\n‚úÖ PCK metric ready!\")"
      ],
      "metadata": {
        "id": "6-ojM8S3RFqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 7: Evaluation Function\n",
        "# ============================================================================\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_on_pf_willow(\n",
        "    model,\n",
        "    dataloader,\n",
        "    device='cuda',\n",
        "    thresholds=[0.05, 0.10, 0.15, 0.20]\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate trained model on PF-WILLOW dataset.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model from Task 2\n",
        "        dataloader: PF-WILLOW DataLoader\n",
        "        device: 'cuda' or 'cpu'\n",
        "        thresholds: PCK thresholds [0.05, 0.10, 0.15, 0.20]\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation results\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize storage\n",
        "    all_results = {f'PCK@{t:.2f}': [] for t in thresholds}\n",
        "    category_results = {}\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"EVALUATING ON PF-WILLOW\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Thresholds: {thresholds}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Total pairs: {len(dataloader)}\\n\")\n",
        "\n",
        "    # Evaluation loop\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        # Move to device\n",
        "        src_img = batch['src_img'].to(device)\n",
        "        tgt_img = batch['tgt_img'].to(device)\n",
        "        src_kps = batch['src_kps'].to(device)\n",
        "        tgt_kps = batch['tgt_kps'].to(device)\n",
        "        valid_mask = batch['valid_mask'].to(device)\n",
        "        category = batch['category'][0]  # batch_size=1\n",
        "        tgt_size = batch['tgt_size'][0]  # (H, W)\n",
        "\n",
        "        # Get valid keypoints\n",
        "        valid_idx = valid_mask[0]\n",
        "\n",
        "        if valid_idx.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        # Predict correspondences\n",
        "        # IMPORTANTE: Il modello deve avere un metodo predict()\n",
        "        pred_kps = model.predict(src_img, tgt_img, src_kps)\n",
        "\n",
        "        # Extract valid predictions\n",
        "        pred_valid = pred_kps[0][valid_idx]  # (N_valid, 2)\n",
        "        gt_valid = tgt_kps[0][valid_idx]     # (N_valid, 2)\n",
        "\n",
        "        # Compute PCK\n",
        "        H, W = tgt_size.tolist()\n",
        "        pck_scores = compute_pck(\n",
        "            pred_valid.cpu(),\n",
        "            gt_valid.cpu(),\n",
        "            (H, W),\n",
        "            thresholds\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        for key, value in pck_scores.items():\n",
        "            all_results[key].append(value)\n",
        "\n",
        "        # Per-category results\n",
        "        if category not in category_results:\n",
        "            category_results[category] = {k: [] for k in pck_scores.keys()}\n",
        "\n",
        "        for key, value in pck_scores.items():\n",
        "            category_results[category][key].append(value)\n",
        "\n",
        "    # Aggregate results\n",
        "    final_results = {\n",
        "        'num_pairs': len(all_results['PCK@0.10']),\n",
        "        'overall': {},\n",
        "        'per_category': {}\n",
        "    }\n",
        "\n",
        "    # Overall metrics\n",
        "    for metric in [f'PCK@{t:.2f}' for t in thresholds]:\n",
        "        values = all_results[metric]\n",
        "        final_results['overall'][metric] = {\n",
        "            'mean': np.mean(values),\n",
        "            'std': np.std(values),\n",
        "            'values': values  # Keep all values for analysis\n",
        "        }\n",
        "\n",
        "    # Per-category metrics\n",
        "    for cat, metrics in category_results.items():\n",
        "        final_results['per_category'][cat] = {}\n",
        "        for metric in [f'PCK@{t:.2f}' for t in thresholds]:\n",
        "            final_results['per_category'][cat][metric] = np.mean(metrics[metric])\n",
        "\n",
        "    return final_results\n",
        "\n",
        "\n",
        "print(\"‚úÖ Evaluation function ready!\")"
      ],
      "metadata": {
        "id": "S2tHq13MRFtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 8: Run Evaluation on PF-WILLOW\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f\" Starting PF-WILLOW evaluation\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Model: Task 2 trained model\")\n",
        "print(f\"   Test pairs: {len(test_dataset)}\\n\")\n",
        "\n",
        "# Run evaluation\n",
        "try:\n",
        "    results = evaluate_on_pf_willow(\n",
        "        model=model,\n",
        "        dataloader=test_loader,\n",
        "        device=device,\n",
        "        thresholds=[0.05, 0.10, 0.15, 0.20]\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"PF-WILLOW EVALUATION RESULTS\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    print(f\"\\n Overall Performance:\")\n",
        "    print(\"-\" * 70)\n",
        "    for metric in ['PCK@0.05', 'PCK@0.10', 'PCK@0.15', 'PCK@0.20']:\n",
        "        mean_val = results['overall'][metric]['mean']\n",
        "        std_val = results['overall'][metric]['std']\n",
        "        print(f\"  {metric}: {mean_val:.2f}% ¬± {std_val:.2f}%\")\n",
        "\n",
        "    print(f\"\\n Per-Category Performance:\")\n",
        "    print(\"-\" * 70)\n",
        "    for cat in sorted(results['per_category'].keys()):\n",
        "        metrics = results['per_category'][cat]\n",
        "        print(f\"\\n  {cat.upper()}:\")\n",
        "        for metric in ['PCK@0.05', 'PCK@0.10', 'PCK@0.15', 'PCK@0.20']:\n",
        "            value = metrics[metric]\n",
        "            print(f\"    {metric}: {value:.2f}%\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "\n",
        "    # Save results\n",
        "    output_file = f'{RESULTS_DIR}/pf_willow_task2_results.json'\n",
        "    with open(output_file, 'w') as f:\n",
        "        # Convert to serializable format\n",
        "        save_results = {\n",
        "            'num_pairs': results['num_pairs'],\n",
        "            'overall': {k: {'mean': v['mean'], 'std': v['std']}\n",
        "                       for k, v in results['overall'].items()},\n",
        "            'per_category': results['per_category']\n",
        "        }\n",
        "        json.dump(save_results, f, indent=2)\n",
        "\n",
        "    print(f\"\\n Results saved to: {output_file}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error during evaluation: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete!\")"
      ],
      "metadata": {
        "id": "2RjvMKUTRFwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 9: Visualizations - PCK Curves\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "def plot_pck_curves(results, save_dir):\n",
        "    \"\"\"Plot PCK curves across different thresholds.\"\"\"\n",
        "\n",
        "    thresholds = [0.05, 0.10, 0.15, 0.20]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # ========================================================================\n",
        "    # Plot 1: Overall PCK Curve\n",
        "    # ========================================================================\n",
        "    ax1 = axes[0]\n",
        "\n",
        "    pck_means = [results['overall'][f'PCK@{t:.2f}']['mean'] for t in thresholds]\n",
        "    pck_stds = [results['overall'][f'PCK@{t:.2f}']['std'] for t in thresholds]\n",
        "\n",
        "    ax1.plot(thresholds, pck_means, marker='o', linewidth=3,\n",
        "             markersize=10, color='#e74c3c', label='Overall PCK')\n",
        "    ax1.fill_between(thresholds,\n",
        "                     [m - s for m, s in zip(pck_means, pck_stds)],\n",
        "                     [m + s for m, s in zip(pck_means, pck_stds)],\n",
        "                     alpha=0.2, color='#e74c3c')\n",
        "\n",
        "    ax1.set_xlabel('Threshold (Œ±)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('PCK (%)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_title('PF-WILLOW: Overall PCK Curve', fontsize=16, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim([0, 100])\n",
        "    ax1.legend(fontsize=12)\n",
        "\n",
        "    # Add value labels\n",
        "    for t, mean in zip(thresholds, pck_means):\n",
        "        ax1.text(t, mean + 2, f'{mean:.1f}%',\n",
        "                ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # ========================================================================\n",
        "    # Plot 2: Per-Category PCK@0.10\n",
        "    # ========================================================================\n",
        "    ax2 = axes[1]\n",
        "\n",
        "    categories = sorted(results['per_category'].keys())\n",
        "    pck_010_per_cat = [results['per_category'][cat]['PCK@0.10'] for cat in categories]\n",
        "\n",
        "    colors = sns.color_palette('husl', len(categories))\n",
        "    bars = ax2.bar(categories, pck_010_per_cat, color=colors, alpha=0.8,\n",
        "                   edgecolor='black', linewidth=1.5)\n",
        "\n",
        "    ax2.set_ylabel('PCK@0.10 (%)', fontsize=14, fontweight='bold')\n",
        "    ax2.set_title('PF-WILLOW: Per-Category Performance', fontsize=16, fontweight='bold')\n",
        "    ax2.set_ylim([0, 100])\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, val in zip(bars, pck_010_per_cat):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
        "                f'{val:.1f}%', ha='center', va='bottom',\n",
        "                fontsize=11, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save\n",
        "    save_path = f\"{save_dir}/pf_willow_pck_analysis.png\"\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\" Saved: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Generate plot\n",
        "if 'results' in locals():\n",
        "    plot_pck_curves(results, FIGURES_DIR)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results available. Run evaluation first!\")"
      ],
      "metadata": {
        "id": "j9zS4mtpRFzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 10: Visualizations - Detailed Heatmap\n",
        "# ============================================================================\n",
        "\n",
        "def plot_detailed_heatmap(results, save_dir):\n",
        "    \"\"\"Plot heatmap of PCK across categories and thresholds.\"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Prepare data\n",
        "    categories = sorted(results['per_category'].keys())\n",
        "    thresholds = [0.05, 0.10, 0.15, 0.20]\n",
        "    threshold_labels = [f'PCK@{t:.2f}' for t in thresholds]\n",
        "\n",
        "    # Create heatmap data\n",
        "    heatmap_data = []\n",
        "    for cat in categories:\n",
        "        row = [results['per_category'][cat][label] for label in threshold_labels]\n",
        "        heatmap_data.append(row)\n",
        "\n",
        "    # Plot heatmap\n",
        "    im = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
        "\n",
        "    # Set ticks\n",
        "    ax.set_xticks(range(len(threshold_labels)))\n",
        "    ax.set_yticks(range(len(categories)))\n",
        "    ax.set_xticklabels(threshold_labels, fontsize=12)\n",
        "    ax.set_yticklabels(categories, fontsize=12)\n",
        "\n",
        "    # Add text annotations\n",
        "    for i in range(len(categories)):\n",
        "        for j in range(len(threshold_labels)):\n",
        "            text = ax.text(j, i, f'{heatmap_data[i][j]:.1f}',\n",
        "                          ha=\"center\", va=\"center\",\n",
        "                          color=\"black\", fontsize=11, fontweight='bold')\n",
        "\n",
        "    ax.set_title('PF-WILLOW: PCK Heatmap (Categories √ó Thresholds)',\n",
        "                fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Colorbar\n",
        "    cbar = plt.colorbar(im, ax=ax)\n",
        "    cbar.set_label('PCK (%)', fontsize=12, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save\n",
        "    save_path = f\"{save_dir}/pf_willow_heatmap.png\"\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\" Saved: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Generate heatmap\n",
        "if 'results' in locals():\n",
        "    plot_detailed_heatmap(results, FIGURES_DIR)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results available. Run evaluation first!\")"
      ],
      "metadata": {
        "id": "LlsyzYr7RF16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 11: Compare SPair-71k vs PF-WILLOW (Zero-Shot Transfer)\n",
        "# ============================================================================\n",
        "\n",
        "print(\" Zero-Shot Transfer Analysis: SPair-71k ‚Üí PF-WILLOW\\n\")\n",
        "\n",
        "# Try to load SPair-71k results if available\n",
        "spair_results_file = f'{RESULTS_DIR}/spair_task2_results.json'\n",
        "\n",
        "if Path(spair_results_file).exists():\n",
        "    with open(spair_results_file, 'r') as f:\n",
        "        spair_results = json.load(f)\n",
        "\n",
        "    print(\"‚úÖ Found SPair-71k results for comparison\\n\")\n",
        "\n",
        "    # Create comparison table\n",
        "    print(f\"{'='*80}\")\n",
        "    print(\"ZERO-SHOT GENERALIZATION ANALYSIS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\n{'Metric':<15} {'SPair-71k (train)':<20} {'PF-WILLOW (zero-shot)':<25} {'Drop':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for metric in ['PCK@0.05', 'PCK@0.10', 'PCK@0.15', 'PCK@0.20']:\n",
        "        if metric in spair_results['overall'] and metric in results['overall']:\n",
        "            spair_val = spair_results['overall'][metric]['mean']\n",
        "            willow_val = results['overall'][metric]['mean']\n",
        "            drop = spair_val - willow_val\n",
        "\n",
        "            print(f\"{metric:<15} {spair_val:>18.2f}% {willow_val:>23.2f}% {drop:>9.2f}%\")\n",
        "\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Visualize comparison\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    thresholds = [0.05, 0.10, 0.15, 0.20]\n",
        "    spair_pck = [spair_results['overall'][f'PCK@{t:.2f}']['mean'] for t in thresholds]\n",
        "    willow_pck = [results['overall'][f'PCK@{t:.2f}']['mean'] for t in thresholds]\n",
        "\n",
        "    x = range(len(thresholds))\n",
        "    width = 0.35\n",
        "\n",
        "    ax.bar([i - width/2 for i in x], spair_pck, width,\n",
        "           label='SPair-71k (train)', color='#3498db', alpha=0.8)\n",
        "    ax.bar([i + width/2 for i in x], willow_pck, width,\n",
        "           label='PF-WILLOW (zero-shot)', color='#e74c3c', alpha=0.8)\n",
        "\n",
        "    ax.set_xlabel('PCK Threshold', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('PCK (%)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Zero-Shot Transfer: SPair-71k ‚Üí PF-WILLOW',\n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([f'{t:.2f}' for t in thresholds])\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.set_ylim([0, 100])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = f\"{FIGURES_DIR}/zero_shot_comparison.png\"\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n Saved: {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  SPair-71k results not found at: {spair_results_file}\")\n",
        "    print(\"   Run Task 2 evaluation on SPair-71k first for comparison\")"
      ],
      "metadata": {
        "id": "q8V0dO6jRF4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 12: Summary & Export\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TASK 4 SUMMARY - PF-WILLOW ZERO-SHOT EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if 'results' in locals():\n",
        "    print(f\"\\n Evaluation Statistics:\")\n",
        "    print(f\"  Total test pairs: {results['num_pairs']}\")\n",
        "    print(f\"  Categories evaluated: {len(results['per_category'])}\")\n",
        "\n",
        "    print(f\"\\n Key Results:\")\n",
        "    for metric in ['PCK@0.05', 'PCK@0.10', 'PCK@0.15', 'PCK@0.20']:\n",
        "        mean_val = results['overall'][metric]['mean']\n",
        "        std_val = results['overall'][metric]['std']\n",
        "        print(f\"  {metric}: {mean_val:.2f}% (¬±{std_val:.2f}%)\")\n",
        "\n",
        "    # Find best and worst categories\n",
        "    pck_010_per_cat = {cat: results['per_category'][cat]['PCK@0.10']\n",
        "                       for cat in results['per_category'].keys()}\n",
        "\n",
        "    best_cat = max(pck_010_per_cat.items(), key=lambda x: x[1])\n",
        "    worst_cat = min(pck_010_per_cat.items(), key=lambda x: x[1])\n",
        "\n",
        "    print(f\"\\n Best category (PCK@0.10):\")\n",
        "    print(f\"  {best_cat[0]}: {best_cat[1]:.2f}%\")\n",
        "\n",
        "    print(f\"\\n Most challenging category (PCK@0.10):\")\n",
        "    print(f\"  {worst_cat[0]}: {worst_cat[1]:.2f}%\")\n",
        "\n",
        "    # Export summary\n",
        "    summary = {\n",
        "        'dataset': 'PF-WILLOW',\n",
        "        'num_pairs': results['num_pairs'],\n",
        "        'model': 'Task 2 Trained Model',\n",
        "        'overall_pck': {k: v['mean'] for k, v in results['overall'].items()},\n",
        "        'best_category': {'name': best_cat[0], 'pck@0.10': best_cat[1]},\n",
        "        'worst_category': {'name': worst_cat[0], 'pck@0.10': worst_cat[1]}\n",
        "    }\n",
        "\n",
        "    summary_file = f'{RESULTS_DIR}/task4_summary.json'\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nüíæ Summary saved to: {summary_file}\")\n",
        "\n",
        "print(f\"\\n All results saved in: {RESULTS_DIR}\")\n",
        "print(f\"All figures saved in: {FIGURES_DIR}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ TASK 4 COMPLETE!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "a0STLlsERF7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELLA 13: Export Results for Report\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "if 'results' in locals():\n",
        "    print(\"üìÑ Creating report-ready tables...\\n\")\n",
        "\n",
        "    # Table 1: Overall Results\n",
        "    overall_df = pd.DataFrame([\n",
        "        {\n",
        "            'Metric': metric,\n",
        "            'Mean (%)': f\"{results['overall'][metric]['mean']:.2f}\",\n",
        "            'Std (%)': f\"{results['overall'][metric]['std']:.2f}\"\n",
        "        }\n",
        "        for metric in ['PCK@0.05', 'PCK@0.10', 'PCK@0.15', 'PCK@0.20']\n",
        "    ])\n",
        "\n",
        "    print(\"Table 1: Overall Performance\")\n",
        "    print(overall_df.to_string(index=False))\n",
        "\n",
        "    # Save as CSV\n",
        "    overall_df.to_csv(f'{RESULTS_DIR}/pf_willow_overall.csv', index=False)\n",
        "    print(f\"\\n Saved: {RESULTS_DIR}/pf_willow_overall.csv\")\n",
        "\n",
        "    # Table 2: Per-Category Results\n",
        "    per_cat_data = []\n",
        "    for cat in sorted(results['per_category'].keys()):\n",
        "        row = {'Category': cat}\n",
        "        for metric in ['PCK@0.05', 'PCK@0.10', 'PCK@0.15', 'PCK@0.20']:\n",
        "            row[metric] = f\"{results['per_category'][cat][metric]:.2f}\"\n",
        "        per_cat_data.append(row)\n",
        "\n",
        "    per_cat_df = pd.DataFrame(per_cat_data)\n",
        "\n",
        "    print(f\"\\nTable 2: Per-Category Performance\")\n",
        "    print(per_cat_df.to_string(index=False))\n",
        "\n",
        "    # Save as CSV\n",
        "    per_cat_df.to_csv(f'{RESULTS_DIR}/pf_willow_per_category.csv', index=False)\n",
        "    print(f\"\\n Saved: {RESULTS_DIR}/pf_willow_per_category.csv\")\n",
        "\n",
        "    print(\"\\n‚úÖ Report tables exported!\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results available. Run evaluation first!\")"
      ],
      "metadata": {
        "id": "5RqR0CH2RF_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}